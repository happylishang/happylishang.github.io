---
layout: default
title: "Binder入门与深入"
description: "Java"
categories: [android,Binder]
tags: [Binder]

---

##总结

Native层Binder进程唤醒的关键是ServiceManager这个单利中转站，Client首先找到ServiceManager，请求ServiceManager帮自己找到目标Binder Stub进程。寻找原理是Linux内核空间所有进程复用。ServiceManager找到目标Binder的node节点，并将node节点的引用节点插入到Client线程的ref树中。之后，Client就很方便的与Binder通信了，ServiceManager维护Native服务List 。Java层面的binderService，是通过AMS进行中转，如果Service没启动，就启动Service，之后进行Publish将新进程的Bidner的代理转发给各个端口，谁需要发给谁，但是5.0已经将权限回收，binderService用途不大。其实Service不是承载体，而是承载体的生命管理接口。


#### 目录
    
> [Binder概述](#sumery_binder)  
> [问题引入原理](#binder_qusetions_index)  
> [ServiceManager化身大管家](#ServiceManager)  
> [Service实现逻辑](#service_part_arch)  
> [    -- Service注册逻辑](#service_self_register)     
> [    -- Service自身服务的实现](#service_self_implement)     
> [Client请求的实现逻辑](#client_part_arch)  
> [    -- 请求ServiceManager获得Service代理](#client_get_proxy)     
> [    -- 请求Service服务自身](#client_service)   
 
    
> [Android应用层对Binder的支持AIDL机制](#java_binder_ref)      
> [    -- 应用场景](#service_self_implement)      
> [    -- 实现原理](#service_self_register)   
> [Binder面试应用](#binder_interview)     
   


<a name="binder_interview"></a>

#### Binder面试问题--来说说Binder 由点及面，由面及里：点击事件如何传递到View，RootView+WindowsManager

如何理解这个Binder：从用法理解原理，而不是从原理理解用法。这大概也是比较合理的学习方式。先会说话，再学语法。Binder用来干嘛的，通信，既然是通信，就是把数据传输给目标。具体到Android就是将当前进程的数据发送给目标进程。Android的实现基本算是C/S架构，所有的核心服务有自己的独立进程。Android基于Linux内核，其进程模型就是Linux进程模型，Linux的进程通信方式完全适用于Android，或者说，就是一模一样的。

 
<a name="sumery_binder"></a>


#### Binder概述

   一句话概括进程通信：进程间的数据传递。

   Binder是Anroid系统里最重要的进程通信方式，很多文章会直接用代码、原理类的文字进行描述，对于接触Android与Linux不是特别深的人来说，特别晦涩难懂，经常是看了这忘了那里，其实探索Binder通信的一条核心就是：Client如何找Server，将请求发送给Server，Server再将结果返回给Client。

   Binder基于OpenBinder，被引入后添加了很多Android特性，比如，在驱动层添加了ServiceManager逻辑，搭建起ServiceManger-Clien-Server框架模型。Android基于Linux内核，其进程管理模型完全沿用了Linux的进程/线程模型，进程划分为用户空间与内核空间，在用户空间，进程间是无法通信的，只有通过内核空间才能传递数据。 Binder自身的意义倾向于通信，只是进程间通信的一种方式，但是在Android系统中，Binder被提到了核心高度，Android基本可以看做基于Binder模型实现的是一种Android RPC模型（远程过程调用协议 Remote Procedure Call Protocal ），即：C/S架构。
   
   Binder只是定义了Android通信模型，至于内部的业务实现，还是要有Server自身来实现，不要把数据传输跟业务处理弄混淆，Android只是基于Binder，搭建了一个C/S通信框架、或者说通信协议。Android基于Linux内核，在Linux中，Binder被看做一个字符设备，Binder驱动会为每个打开Binder的进程在内核里分配一块地址空间，Client向Server传递数据，其实就是将数据写道内核空间中Server的地址里面，然后通知Server去取数据。原理其实很简单，但是Google为了更加合理的使用Binder，自己进行了很多层次的封装与优化，导致代码看的昏头转向， 比较难的就是进程或者线程的挂起与唤醒以及Android CS框架。

<a name="binder_qusetions_index"></a>

#### Binder常见问题--由表及里引入原理探究
	
*  ServiceManager如何管理Servers

   每个Server进程在注册的时候，首先往本地进程的内核空间的Binders红黑树种插入Binder实体服务的bind_node节点，然后会在ServiceManager的进程的内核空间中为其添加引用ref结构体，ref，会保存相应的信息、名字、ptr地址等。

*   Client如何找到Server，并且向其发送请求
    Client在getService的时候，ServiceManager会找到Server的node节点，并在在Client中创建Server的bind_ref引用，Client可以在自己进程的内核空间中找到该引用，最终获取Server的bind_node节点，直接访问Server，传输数据并唤醒。

*  Client端，服务实体的引用bind_ref存在哪里了，与Handler的关系式怎么样的

	Binder驱动会在内核空间为打开Binder设备的进程（包括Client及Server端）创建bind_proc结构体，bind_proc包含4棵红黑树：threads、bind_refs、bind_nodes、bind_desc这四棵树分别记录该进程的线程树、Binder引用树、本地Binder实体，等信息，方便本地查找。Handler其是ServiceManager为了方便客户端查找bind_ref做的一套处理，只是为了标定目标。
	
* 如何唤醒目标进程或者线程：
	
	每个Binder进程或者线程在内核中都设置了自己的等待队列，Client将目标进程或者线程告诉Binder驱动，驱动负责唤醒挂起在等待队列上的线程或者进程。

* 	Server如何找到返回目标进程或者线程，Client在请求的时候，会在bind_trasaction的from中添加请求端信息

* 	如何Binder节点与ref节点的添加时机 
	
	驱动中存在一个TYPE_BINDER与TYPR_HANDLE的转换，Binder节点是Binder Server进程（一般是Native进程）在向Servicemanager注册时候添加的，而ref是Client在getService的时候添加的，并且是由ServiceManager添加的。

* 	Binder如何实现只拷贝一次

	数据从用户空间拷贝到内核中的时候，是直接拷贝到目标进程的内核空间，这个过程是在请求端线程中处理的，只不过操作对象是目标进城的内核空间。其实，内核中的bind_trasaction_data是直接在目标进程汇总分配的，由于Binder进程的Binder内存部分在内核空间跟用户空间只存在一个偏差值，用户空间不需要再次拷贝数据就可
以完成访问。

* 	Binder接收线程管理：请求发送时没有特别标记，驱动怎么判断哪些数据包该送入全局to-do队列，哪些数据包该送入特定线程的to-do队列呢？这里有两条规则：【1】

	规则1：Client发给Server的请求数据包都提交到Server进程的全局to-do队列。不过有个特例，当进程P1的中的线程T1向进程P2发送请求时，驱动会先查看一下线程T1是否也正在处理来自P2某个线程请求，（尚在处理，未完成，没有发送回复），这种情况通常发生在两个进程都有Binder实体并互相对发时请求的时候。如果在进程P2中发现了这样的线程，比如说T2，就会要求T2来处理T1的这次请求。因为T2既然向T1发送了请求尚未得到返回包，说明T2肯定（或将会）阻塞在读取返回包的状态。这时候可以让T2顺便做点事情，总比等在那里闲着好。而且如果T2不是线程池中的线程还可以为线程池分担部分工。经过优化，来自T1的请求不是提交给P2的全局to-do队列，而是送入了T2的私有to-do队列。

	规则2：对同步请求的返回数据包（由BC_REPLY发送的包）都发送到发起请求的线程的私有to-do队列中。如上面的例子，如果进程P1的线程T1发给进程P2的线程T2的是同步请求，那么T2返回的数据包将送进T1的私有to-do队列而不会提交到P1的全局to-do队列。

* Binder Server都会在ServiceManager中注册吗？

	Java层的Binder实体就不会去ServiceManager，尤其是bindService这样一种，其实是ActivityManagerService充当了ServiceManager的角色。

* IPCThreadState::joinThreadPool的真正意义是什么？
	
	可以理解加入该进程内核的线程池，进行循环，多个线程开启，其实一个就可以，怕处理不过来，可以开启多个线程处理起来，其实跟线程池类似。
	
* 为何ServiceManager启动的时候没有采用joinThreadPool，而是自己通过for循环来实现自己Loop

  因为Binder环境还没准备好啊，所以，自己控制，所以也咩有talkWithDriver那套逻辑，不用onTransact实现。因为前文也说过，Binder为Android做了深层的改变，其实在驱动里面ServiceManager也是特殊对待的，在binder_transaction中，会对目标是ServiceManager的请求进行特殊处理。	

<a name="become_service_manager"></a>

#### ServiceManager启动为管家

##### ServiceManager是由谁启动的？

 在应用层ServiceManager的使用一般如下:(基于源码4.3)

    public abstract Object getSystemService(@ServiceName @NonNull String name);
    
 那么ServiceManager是什么时候启动的呢？ServiceManager代码位于/frameworks/native/cmds/servicemanager/中，在init.rc中可以看到
    
	    service servicemanager /system/bin/servicemanager
	    class core
	    user system
	    group system
	    critical
	    onrestart restart zygote
	    onrestart restart media
	    onrestart restart surfaceflinger
	    onrestart restart drm

所以，ServiceManager是有init进程启动的，在Linux系统中init是一切用户空间进程的父进程，ServiceManager因此不依赖与任何Android服务进程。完全由系统的init进程加载进来。

##### ServiceManager如何成为系统Server的大管家

init进程启动的servicemanager的入口是service_manager.c的main函数：

		int main(int argc, char **argv)
		{
		    struct binder_state *bs;
		    void *svcmgr = BINDER_SERVICE_MANAGER;
		    bs = binder_open(128*1024);
		    if (binder_become_context_manager(bs)) {
		        LOGE("cannot become context manager (%s)\n", strerror(errno));
		        return -1;
		    }
		    svcmgr_handle = svcmgr;
		    binder_loop(bs, svcmgr_handler);
		    return 0;
		}

主要做了以下几件事情		

* 	第一步：调用函数binder_open打开设备文件/dev/binder 
* 	第二步：调用binder_become_context_manager将自己注册为Binder进程间通信机制的上下文管理者；
* 	第三步：调用函数binder_loop开启循环，监听Client进程的通信要求。
下面详细的分析一下核心函数代码：

	int binder_become_context_manager(struct binder_state *bs)
	{
	    return ioctl(bs->fd, BINDER_SET_CONTEXT_MGR, 0);
	}

进入binder驱动

	static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
	{
	     int ret;
	    struct binder_proc *proc = filp->private_data;
	    struct binder_thread *thread;
	    unsigned int size = _IOC_SIZE(cmd);
	    void __user *ubuf = (void __user *)arg;
	    
	    . . . . . .
	    . . . . . .
	    case BINDER_SET_CONTEXT_MGR:
	        . . . . . .
	        . . . . . .
	            binder_context_mgr_uid = current->cred->euid;
	        
	        binder_context_mgr_node = binder_new_node(proc, NULL, NULL);
	 if (binder_context_mgr_node == NULL) 
	        {
	            ret = -ENOMEM;
	            goto err;
	        }
	        binder_context_mgr_node->local_weak_refs++;
	        binder_context_mgr_node->local_strong_refs++;
	        binder_context_mgr_node->has_strong_ref = 1;
	        binder_context_mgr_node->has_weak_ref = 1;
	 break;
	    . . . . . .
	    . . . . . .
	}
Binder驱动为ServiceManager生成一个binder_node节点，并记入静态变量binder_context_mgr_node。一般情况下，应用层的每个binder实体都会在binder驱动层对应一个binder_node节点，然而binder_context_mgr_node比较特殊，它没有对应的应用层binder实体。系统规定：任何应用都必须使用句柄0来跨进程地访问它，因为ServiceManager独一无二，并且只有一个服务实体，所以并不需要区分Binder实体。Android规定几乎任何用户进程都可以通过0号句柄访问ServiceManager，其余Server的远程接口句柄之都是一个Binder驱动分配的大于0的值，可以在Binder驱动中看到对于请求是ServiceManager的特殊处理

关于binder_loop开启循环

	void binder_loop(struct binder_state *bs, binder_handler func)
	{
	    int res;
	    struct binder_write_read bwr;
	    unsigned readbuf[32];
	
	    bwr.write_size = 0;
	    bwr.write_consumed = 0;
	    bwr.write_buffer = 0;
	    
	    readbuf[0] = BC_ENTER_LOOPER;
	    binder_write(bs, readbuf, sizeof(unsigned));
	    for (;;) {
	        bwr.read_size = sizeof(readbuf);
	        bwr.read_consumed = 0;
	        bwr.read_buffer = (unsigned) readbuf;
	        res = ioctl(bs->fd, BINDER_WRITE_READ, &bwr);
	        res = binder_parse(bs, 0, readbuf, bwr.read_consumed, func);
	  		....
	    }
	}

可以看出ServiceManager直接通过for的ioctl进行binder字符设备的读取，如果没请求到来，就将自己的进程挂起，等待请求唤醒。一般Client的请求目标如果是ServiceManager会进行如下区分处理：
 
	 if (tr->target.handle) {//如果不是ServiceManager，也就是tr->target.handle！=0
					struct binder_ref *ref;
					ref = binder_get_ref(proc, tr->target.handle);
					if (ref == NULL) {
						binder_user_error("binder: %d:%d got "
							"transaction to invalid handle\n",
							proc->pid, thread->pid);
						return_error = BR_FAILED_REPLY;
						goto err_invalid_target_handle;
					}
					target_node = ref->node;
				} else {//如果不是ServiceManager，也就是tr->target.handle==0
					target_node = binder_context_mgr_node;
					if (target_node == NULL) {
						return_error = BR_DEAD_REPLY;
						goto err_no_context_mgr_node;
					}
				
到这里我们就知道，ServiceManager怎么启动、究竟做什么，以及客户端如何找到ServiceManager的逻辑。下面就来看一下Server如何将自己注册到ServiceManager中，以及Client如何通过，。ServiceManager找到目标Server。			
					
<a name="service_part_arch"></a>
					
####  系统Server实现逻辑 

					
<a name="service_self_register"></a> 

##### Server（Native层）如何注册到ServiceManager？

系统启动的时候，init进程会先启动ServicManager进程，之后，init会启动mediaserver进程，注意mediaserver不是SysytemServer启动的，而是init启动的。MediaPlayerServiceService的，SystemServer.java通过init1调用Jni函数，mediaserver配置在init.rc配置文件中，init.rc中的Service服务进程是顺序启动的。

	service media /system/bin/mediaserver
	    class main
	    user media
	    group audio camera inet net_bt net_bt_admin net_bw_acct drmrpc mediadrm
	    ioprio rt 4
	    
mediaserver的入口是/frameworks/av/media/mediaserver/main_mediaserver.cpp的main函数：

		int main(int argc, char** argv){
		
		    sp<ProcessState> proc(ProcessState::self());
		
		    sp<IServiceManager> sm = defaultServiceManager();
		
		    AudioFlinger::instantiate(); //内含注册到ServiceManager
		    MediaPlayerService::instantiate();//内含注册到ServiceManager
		    CameraService::instantiate();//内含注册到ServiceManager
		    AudioPolicyService::instantiate();//内含注册到ServiceManager
		
		    ProcessState::self()->startThreadPool();
		    IPCThreadState::self()->joinThreadPool();
		}

通过以上代码，可以看出一个进程可以同时注册了多个服务，那么到底是怎么注册到ServiceManager呢。主观上想就是通过defaultServiceManager()获取ServiceManager的远程代理，然后通过代理发送请求，将服务Server信息注册到ServiceManager中。

###### defaultServiceManager()函数如何获取ServiceManager的远程代理呢？

defaultServiceManager函数定义在IServiceManager.h中，但是不属于某个类，类似全局方法，其实现也透漏着单利模式的影子：

	sp<IServiceManager> defaultServiceManager()
	{
	    if (gDefaultServiceManager != NULL) return gDefaultServiceManager;
	    {
	        AutoMutex _l(gDefaultServiceManagerLock);
	        if (gDefaultServiceManager == NULL) {
	            gDefaultServiceManager = interface_cast<IServiceManager>(
	                ProcessState::self()->getContextObject(NULL));
	        }
	    }
	    return gDefaultServiceManager;
	}
	
gDefaultServiceManager定义在namespace android中，是个全局变量，如果非要找源码，可以再static.h中找到它

	namespace android {
	// For ProcessState.cpp
	extern Mutex gProcessMutex;
	extern sp<ProcessState> gProcess;
	// For ServiceManager.cpp
	extern Mutex gDefaultServiceManagerLock;
	extern sp<IServiceManager> gDefaultServiceManager;
	extern sp<IPermissionController> gPermissionController;
	}// namespace android
	
在IServiceManager /ProcessState.cpp中发现#include <private/binder/Static.h>引入这些单利模式对象，到这里可以知道defaultServiceManager()函数返回的gDefaultServiceManager其实是一个IServiceManager实例，这个实例是通过interface_cast<IServiceManager>(.. .}转化来的，interface_cast是内联函数，定义在interface.h中

	template<typename INTERFACE>
	inline sp<INTERFACE> interface_cast(const sp<IBinder>& obj)
	{
	    return INTERFACE::asInterface(obj);
	}
	
这是个泛型函数，整理一下就是

	IServiceManager::asInterface(ProcessState::self()->getContextObject(NULL));
	
asInterface的作用这里强调一下：**是为了抽象业务**，通过 DECLARE_META_INTERFACE(INTERFACE)定义一些INTERFACE相关的函数，这种处理方式实现了代码的解耦合与复用。如果想实现哪个Service只需要DECLARE_META_INTERFACE即可，比如DECLARE_META_INTERFACE(MediaPlayerService)：

	#define DECLARE_META_INTERFACE(INTERFACE)                               \
	    static const android::String16 descriptor;                          \
	    static android::sp<I##INTERFACE> asInterface(                       \
	            const android::sp<android::IBinder>& obj);                  \
	    virtual const android::String16& getInterfaceDescriptor() const;    \
	    I##INTERFACE();                                                     \
	    virtual ~I##INTERFACE(); 
	 IServiceManager::asInterface通过宏来定义与实现
	    android::sp<I##INTERFACE> I##INTERFACE::asInterface(                \
	            const android::sp<android::IBinder>& obj)                   \
	    {                                                                   \
	        android::sp<I##INTERFACE> intr;                                 \
	        if (obj != NULL) {                                              \
	            intr = static_cast<I##INTERFACE*>(                          \
	                obj->queryLocalInterface(                               \
	                        I##INTERFACE::descriptor).get());               \
	            if (intr == NULL) {                                         \
	                intr = new Bp##INTERFACE(obj);                          \
	            }                                                           \
	        }                                                               \
	        return intr;                                                    \
	    }                                                                   \
	    I##INTERFACE::I##INTERFACE() { }                                    \
	    I##INTERFACE::~I##INTERFACE() { }  
	    
	    
这里**##**的作用是连接符，将两个参数成为一个整体， obj->queryLocalInterface(I##INTERFACE::descriptor).get())的作用是查询Service是否位于当前进程 如果是，就返回Service自身，如果跨进程，就返回相应的是Bp##INTERFACE代理，这里看到obj的类型是IBinder型sp，而ProcessState::self()->getContextObject(NULL)返回也确实是一个BpBinder，详细看一下：

	sp<IBinder> ProcessState::getContextObject(const sp<IBinder>& caller)
	{
	    return getStrongProxyForHandle(0);
	}

	<!--查找索引-->
	sp<IBinder> ProcessState::getStrongProxyForHandle(int32_t handle)
	{
	    sp<IBinder> result;
	    AutoMutex _l(mLock);
		handle_entry* e = lookupHandleLocked(handle);--》 查找对应索引的资源,看看是否有缓存（曾经用过）	     if (e != NULL) {
	        IBinder* b = e->binder; -->第一次进来必定为空
	        if (b == NULL || !e->refs->attemptIncWeak(this)) {
	            b = new BpBinder(handle); --->创建了一个新的BpBinder，并且handle是0
	            e->binder = b;
	            result = b;
	        } 
	    }
	    return result; 返回BpBinder(0)。
	}
	
	<!--BpBinder的意义在于通信-->
	
	BpBinder::BpBinder(int32_t handle)
	    : mHandle(handle)
	    , mAlive(1)
	    , mObitsSent(0)
	    , mObituaries(NULL)
	{
	 	extendObjectLifetime(OBJECT_LIFETIME_WEAK);
	    IPCThreadState::self()->incWeakHandle(handle);
	}
	
 IPCThreadState类的成员函数incWeakHandle将添加Binder引用对象的弱引用计数的操作缓存在内部的一个成员变量mOut中，等到下次使用IO控制命令BINDER_WRITE_READ进入到Binder驱动程序时，再请求Binder驱动程序添加对应的Binder引用对象的弱引用计数。最后简化为

	gDefaultServiceManager =BpServiceManager (new BpBinder(0));
	
到此，获取了ServiceManager的本地代理，

	class BpServiceManager : public BpInterface<IServiceManager>
	{
	public:
	BpServiceManager(const sp<IBinder>& impl)
	        : BpInterface<IServiceManager>(impl) { }
	virtual sp<IBinder> getService(const String16& name) const {
	        unsigned n;
	        for (n = 0; n < 5; n++){
	            sp<IBinder> svc = checkService(name);
	            if (svc != NULL) return svc;
	            LOGI("Waiting for service %s...\n", String8(name).string());
	            sleep(1);
	        }
	        return NULL;
	}
	
###### BpInterface采用了泛型，统一了业务接口的封装接口

	template<typename INTERFACE>
	class BpInterface : public INTERFACE, public BpRefBase
	{
	public:  	BpInterface(const sp<IBinder>& remote);
	protected:  virtual IBinder*      onAsBinder();
	};
	
	
	template<typename INTERFACE>
	inline BpInterface<INTERFACE>::BpInterface(const sp<IBinder>& remote) : BpRefBase(remote){}

	template<typename INTERFACE>
	inline IBinder* BpInterface<INTERFACE>::onAsBinder()
	{
	    return remote();
	}


	BpRefBase::BpRefBase(const sp<IBinder>& o)
	    : mRemote(o.get()), mRefs(NULL), mState(0)
	{
	    extendObjectLifetime(OBJECT_LIFETIME_WEAK);
	
	    if (mRemote) {
	        mRemote->incStrong(this);           // Removed on first IncStrong().
	        mRefs = mRemote->createWeak(this);  // Held for our entire lifetime.
	    }
	}

用于通信的BpBinder最终被赋值给mRemote，这牵扯到Android智能指针的问题，有时间大家自己当做一个专题去学习一下， 到此，就真正完成了BpServiceManager的创建于分析。既然创建好了，那么用一下，去ServiceManager登记当前系统Server

	   defaultServiceManager()->addService(String16("media.player"), new MediaPlayerService());
	   
这里其实就是调用BpServiceManager的addService，看一下源码：

	virtual status_t addService(const String16& name, const sp<IBinder>& service)
	{
	    Parcel data, reply;
	    data.writeInterfaceToken(IServiceManager::getInterfaceDescriptor());
	    data.writeString16(name);
	    data.writeStrongBinder(service);
	    status_t err = remote()->transact(ADD_SERVICE_TRANSACTION, data, &reply);
	    return err == NO_ERROR ? reply.readExceptionCode() : err;
	}
	
关键是remote()->transact(ADD_SERVICE_TRANSACTION, data, &reply)，而remote()最终返回的其实就是mRemote，也就是BpBinder(0)，

	inline  IBinder*        remote()                { return mRemote; }
	
再来看一下BpBinder的transact函数，

	status_t BpBinder::transact(uint32_t code, const Parcel& data, Parcel* reply, uint32_t flags){
	 if (mAlive) {
	        status_t status = IPCThreadState::self()->transact(mHandle, code, data, reply, flags);
	        if (status == DEAD_OBJECT) mAlive = 0;
	        return status;   }
	    return DEAD_OBJECT;
	}
可以看出最终调用：

    status_t status = IPCThreadState::self()->transact(mHandle, code, data, reply, flags);
    
我们看一下关键代码：

	status_t IPCThreadState::transact(int32_t handle, uint32_t code, const Parcel& data,  Parcel* reply, uint32_t flags)
	{
	    status_t err = data.errorCheck();
	    flags |= TF_ACCEPT_FDS;
	    if (err == NO_ERROR) {
	    // 写缓存
	        err = writeTransactionData(BC_TRANSACTION, flags, handle, code, data, NULL); //写缓存 mout
	    } 
	    if ((flags & TF_ONE_WAY) == 0) {
	        if (reply) {
	            err = waitForResponse(reply); // 访问Binder驱动，交互
	        } else {
	            Parcel fakeReply;
	            err = waitForResponse(&fakeReply);
	        }
	    } else {
	        err = waitForResponse(NULL, NULL);
	    }
	    return err;
	}
	
waitForResponse向ServiceManager发送请求，并阻塞等待回复，如何发送的呢：talkWithDriver，

	status_t IPCThreadState::waitForResponse(Parcel *reply, status_t *acquireResult){
	    int32_t cmd;
	    int32_t err;
	    while (1) {
	    
	    <!--这里去驱动发送数据-->
	    
	        if ((err=talkWithDriver()) < NO_ERROR) break;
	    
	        err = mIn.errorCheck();
	        if (err < NO_ERROR) break;
	        if (mIn.dataAvail() == 0) continue;
	        cmd = mIn.readInt32();
	        IF_LOG_COMMANDS() {
	            alog << "Processing waitForResponse Command: "
	                << getReturnString(cmd) << endl;
	        }
	        switch (cmd) {
	        case BR_TRANSACTION_COMPLETE:
	            if (!reply && !acquireResult) goto finish;
	            break;
	            case BR_DEAD_REPLY:
	            err = DEAD_OBJECT;
	            goto finish;
	        case BR_FAILED_REPLY:
	            err = FAILED_TRANSACTION;
	            goto finish;
	        case BR_ACQUIRE_RESULT: {
	                LOG_ASSERT(acquireResult != NULL, "Unexpected brACQUIRE_RESULT");
	                const int32_t result = mIn.readInt32();
	                if (!acquireResult) continue;
	                *acquireResult = result ? NO_ERROR : INVALID_OPERATION;
	            }
	            goto finish; 
	        case BR_REPLY:
	            {
	                binder_transaction_data tr;
	                err = mIn.read(&tr, sizeof(tr));
	                if (reply) {
	                    if ((tr.flags & TF_STATUS_CODE) == 0) {
	                        reply->ipcSetDataReference(
	                            reinterpret_cast<const uint8_t*>(tr.data.ptr.buffer),
	                            tr.data_size,
	                            reinterpret_cast<const size_t*>(tr.data.ptr.offsets),
	                            tr.offsets_size/sizeof(size_t),
	                            freeBuffer, this);
	                    } else {
	                        err = *static_cast<const status_t*>(tr.data.ptr.buffer);
	                        freeBuffer(NULL, reinterpret_cast<const uint8_t*>(tr.data.ptr.buffer),tr.data_size,
	                            reinterpret_cast<const size_t*>(tr.data.ptr.offsets),tr.offsets_size/sizeof(size_t), this);
	                    }} else {
	                    freeBuffer(NULL,
	                        reinterpret_cast<const uint8_t*>(tr.data.ptr.buffer),
	                        tr.data_size,
	                        reinterpret_cast<const size_t*>(tr.data.ptr.offsets),
	                        tr.offsets_size/sizeof(size_t), this);
	                    continue; }}
	            goto finish; 
	        default:
	            err = executeCommand(cmd);
	            if (err != NO_ERROR) goto finish;
	            break; }}
	finish:
	    if (err != NO_ERROR) {
	        if (acquireResult) *acquireResult = err;
	        if (reply) reply->setError(err);
	        mLastError = err;
	    } 
	    return err;
	}
	
talkWithDriver()的涉及到ioctrl，去访问Binder驱动，这里牵扯的驱动的问题。

	status_t IPCThreadState::talkWithDriver(bool doReceive) {  
	    binder_write_read bwr;    
	    // Is the read buffer empty?
	    const bool needRead = mIn.dataPosition() >= mIn.dataSize();    
	    // We don't want to write anything if we are still reading
	    // from data left in the input buffer and the caller
	    // has requested to read the next data.
	    //正在读取的时候不写
	    const size_t outAvail = (!doReceive || needRead) ? mOut.dataSize() : 0;    
	    bwr.write_size = outAvail;
	    bwr.write_buffer = (long unsigned int)mOut.data();
	    // This is what we'll read.
	    if (doReceive && needRead) {
	        bwr.read_size = mIn.dataCapacity();
	        bwr.read_buffer = (long unsigned int)mIn.data();
	    } else {
	        bwr.read_size = 0;
	    }    
	    // Return immediately if there is nothing to do.
	    if ((bwr.write_size == 0) && (bwr.read_size == 0)) return NO_ERROR;    
	    bwr.write_consumed = 0;
	    bwr.read_consumed = 0;
	    status_t err;
	    do {
	        if (ioctl(mProcess->mDriverFD, BINDER_WRITE_READ, &bwr) >= 0)
	            err = NO_ERROR;
	        else
	            err = -errno；
	        IF_LOG_COMMANDS() {
	            alog << "Finished read/write, write size = " << mOut.dataSize() << endl;
	        }//这个log说明，talkWithDriver是读写一体的，并且，请求段是采用阻塞的方式来等待请求返回的
	    } while (err == -EINTR);
	    if (err >= NO_ERROR) {
	        if (bwr.write_consumed > 0) {
	            if (bwr.write_consumed < (ssize_t)mOut.dataSize())
	                mOut.remove(0, bwr.write_consumed);
	            else
	                mOut.setDataSize(0);
	        }
	        if (bwr.read_consumed > 0) {//通知去读
	            mIn.setDataSize(bwr.read_consumed);
	            mIn.setDataPosition(0);
	        }
	        return NO_ERROR;
	    }    
	    return err;
	}
	
看ioctrl，系统调用函数，其对应的bind_ioctrl位于Binder驱动中，首先会写数据，如果是异步传输，不需要等待回复数据，如果是同步请求，需要阻塞等在数据返回。在binder_transcation_data中的flags域中可以体现出来，也就是flags的TF_ONE_WAY位为1，就表示需要异步传输。
	
	static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
	{
		int ret;
		// 当前进程对应的binder_proc 
		struct binder_proc *proc = filp->private_data;
		// 进程的每个线程在binder驱动中的表示
		struct binder_thread *thread;
		unsigned int size = _IOC_SIZE(cmd);
		void __user *ubuf = (void __user *)arg;
		binder_lock(__func__);
		thread = binder_get_thread(proc);
		if (thread == NULL) {
			ret = -ENOMEM;
			goto err;
		}
		switch (cmd) {
		case BINDER_WRITE_READ: {
			struct binder_write_read bwr;
			if (size != sizeof(struct binder_write_read)) {
				ret = -EINVAL;
				goto err;
			}
			if (copy_from_user(&bwr, ubuf, sizeof(bwr))) {
				ret = -EFAULT;
				goto err;
			}

			//  > 0, 表示本次ioctl有待发送的数据
			if (bwr.write_size > 0) {
				// 写数据
				ret = binder_thread_write(proc, thread, (void __user *)bwr.write_buffer, bwr.write_size, &bwr.write_consumed);
				trace_binder_write_done(ret);
				// 成功返回0，出错小于0
				if (ret < 0) {
					bwr.read_consumed = 0;
					if (copy_to_user(ubuf, &bwr, sizeof(bwr)))
						ret = -EFAULT;
					goto err;
				}
			}
			//> 0表示本次ioctl想接收数据
			if (bwr.read_size > 0) {
				// binder驱动接收读数据函数，这里会阻塞，然后被唤醒
				ret = binder_thread_read(proc, thread, (void __user *)bwr.read_buffer, bwr.read_size, &bwr.read_consumed, filp->f_flags & O_NONBLOCK);
				trace_binder_read_done(ret);
				 /* 读返回的时候如果发现todo任务队列中有待处理的任务，那么将会唤醒binder_proc.wait中下一个等待着的空闲线程。*/
				if (!list_empty(&proc->todo))
					wake_up_interruptible(&proc->wait);
				if (ret < 0) {
					if (copy_to_user(ubuf, &bwr, sizeof(bwr)))
						ret = -EFAULT;
					goto err;
				}
			}			
			
在写请求与数据的过程中，会调用binder_transaction函数，这个函数是比较关键的函数，里面涉及到Binder驱动管理的核心部分，包括:

* 找到目标进程或者线程
* Binder数据一次拷贝原理
* 目标线程或者进程的唤醒  

	
##### binder_transaction如何找到目标进程或者线程--要么是ServiceManager，要么是通过getService获得的引用

	static void binder_transaction(struct binder_proc *proc,
				       struct binder_thread *thread,
				       struct binder_transaction_data *tr, int reply)
	{
		// // 发送请求时使用的内核传输数据结构
		struct binder_transaction *t;
		struct binder_work *tcomplete;
		size_t *offp, *off_end;
		 // 目标进程对应的binder_proc
		struct binder_proc *target_proc;
		// 目标线程对应的binder_thread
		struct binder_thread *target_thread = NULL;
		//目标binder实体在内核中的节点结构体
		struct binder_node *target_node = NULL;
		// 
		struct list_head *target_list;
		wait_queue_head_t *target_wait;
		struct binder_transaction *in_reply_to = NULL;
		struct binder_transaction_log_entry *e;
		uint32_t return_error;
		e = binder_transaction_log_add(&binder_transaction_log);
		e->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);
		e->from_proc = proc->pid;
		e->from_thread = thread->pid;
		e->target_handle = tr->target.handle;
		e->data_size = tr->data_size;
		e->offsets_size = tr->offsets_size;
		// 如过是返回
		if (reply) {
			// 这里的in_reply_to其实是在Server进程或者线程
			in_reply_to = thread->transaction_stack;
		   // ￥
			binder_set_nice(in_reply_to->saved_priority);
			// 这里是判断是否是正确的返回线程
			if (in_reply_to->to_thread != thread) {
	         // ￥
			}
			thread->transaction_stack = in_reply_to->to_parent;
			// 获取目标线程，这里是在请求发送是写入的
			target_thread = in_reply_to->from;
	 
			if (target_thread->transaction_stack != in_reply_to) {
	        //$
			}
			// 根据目标线程获取目标进程
			target_proc = target_thread->proc;
		} 
		   // 如过是请求
		else {
			// 如果这里目标服务是普通服务 不是SMgr管理类进程 
			if (tr->target.handle) {
				struct binder_ref *ref;
		    // 从proc进程中，这里其实是自己的进程，查询出bind_node的引用bind_ref,
			// 对于一般的进程，会在getService的时候有Servicemanager为自己添加
			// 注意handle 是记录在本地的，用来本地索引ref用的 
				ref = binder_get_ref(proc, tr->target.handle);
			//￥
				target_node = ref->node;
			} else {
			// 如果这里目标服务是Servicemanager服务 
			// 如果ioctl想和SMgr的binder实体建立联系，需要使tr->target.handle = 0
				target_node = binder_context_mgr_node;
		    
			}	
			 e->to_node = target_node->debug_id;
				// 获取目标进程
			 target_proc = target_node->proc;
				
可以看到，如果是请求，会根据target.handle在本进程中找到目标进程的proc节点，当然，第一次一般是找到binder_context_mgr_node，然后binder_context_mgr_node会在Client进程中设置目标Service节点的ref，这样Client就可以找到target_proc了。

##### Binder数据一次拷贝原理--直接将数据从Client的用户空间，拷贝到目标进程的内核空间

 
			//*
			在通过进行binder事物的传递时，如果一个binder事物（用struct binder_transaction结构体表示）需要使用到内存，
			就会调用binder_alloc_buf函数分配此次binder事物需要的内存空间。
			需要注意的是：从目标进程所在binder内存空间分配所需的内存
			//
			//从target进程的binder内存空间分配所需的内存大小,这也是一次拷贝，完成通信的关键，直接拷贝到目标进程的内核空间
			//由于用户空间跟内核空间仅仅存在一个偏移地址，所以也算拷贝到用户空间
			t->buffer = binder_alloc_buf(target_proc, tr->data_size,
				tr->offsets_size, !reply && (t->flags & TF_ONE_WAY));
		    //￥
			t->buffer->allow_user_free = 0;
			t->buffer->debug_id = t->debug_id;
			//该binder_buffer对应的事务    
			t->buffer->transaction = t;
			//该事物对应的目标binder实体 ,因为目标进程中可能不仅仅有一个Binder实体
			t->buffer->target_node = target_node;
		
			trace_binder_transaction_alloc_buf(t->buffer);
			// 
			if (target_node)
				binder_inc_node(target_node, 1, 0, NULL);
			// 计算出存放flat_binder_object结构体偏移数组的起始地址，4字节对齐。
			offp = (size_t *)(t->buffer->data + ALIGN(tr->data_size, sizeof(void *)));
			   // struct flat_binder_object是binder在进程之间传输的表示方式 //
		       // 这里就是完成binder通讯单边时候在用户进程同内核buffer之间的一次拷贝动作 //
			  // 这里的数据拷贝，其实是拷贝到目标进程中去，因为t本身就是在目标进程的内核空间中分配的，
			if (copy_from_user(t->buffer->data, tr->data.ptr.buffer, tr->data_size)) {
				binder_user_error("binder: %d:%d got transaction with invalid "
					"data ptr\n", proc->pid, thread->pid);
				return_error = BR_FAILED_REPLY;
				goto err_copy_data_failed;
			}
			 // 拷贝内嵌在传输数据中的flat_binder_object结构体偏移数组
			if (copy_from_user(offp, tr->data.ptr.offsets, tr->offsets_size)) {
		    //￥
			}
			if (!IS_ALIGNED(tr->offsets_size, sizeof(size_t))) {
		 	// ￥
			}
			//这里不一定一定执行，比如 MediaPlayer getMediaplayerservie的时候，传递数据中不包含对象, off_end为null 
			// 这里就不会执行，一定要携带传输的数据才会走到这里
			//off_end = (void *)offp + tr->offsets_size; 
			//flat_binder_object结构体偏移数组的结束地址
			off_end = (void *)offp + tr->offsets_size;
			for (; offp < off_end; offp++) {
				struct flat_binder_object *fp;
		   // *offp是t->buffer中第一个flat_binder_object结构体的位置偏移,相
		   当于t->buffer->data的偏移，这里的if语句用来判断binder偏移数组的第一个
		   元素所指向的flat_binder_object结构体地址是否是在t->buffer->data的有效范
		   围内，或者数据的总大小就小于一个flat_binder_object的大小，或者说这个数组的元
		   素根本就没有4字节对齐(一个指针在32位平台上用4个字节表示)。//
				if (*offp > t->buffer->data_size - sizeof(*fp) ||
				    t->buffer->data_size < sizeof(*fp) ||
				    !IS_ALIGNED(*offp, sizeof(void *))) {
					 // $
				}
				// 取得第一个flat_binder_object结构体指针
				fp = (struct flat_binder_object *)(t->buffer->data + *offp);
				switch (fp->type) {
		        // 只有具有binder实体的进程才有权利发送这类binder。这里其实是在binder实体中
				case BINDER_TYPE_BINDER:
				case BINDER_TYPE_WEAK_BINDER: {
					struct binder_ref *ref;
					// 根据flat_binder_object.binder这个binder实体在进程间的地
					// 址搜索当前进程的binder_proc->nodes红黑树，看看是否已经创建了binder_node内核节点
					// 如果没创建，在自己的进程中为自己创建node节点 
					struct binder_node *node = binder_get_node(proc, fp->binder);
					if (node == NULL) {
						// 如果为空，创建
						node = binder_new_node(proc, fp->binder, fp->cookie);
						if (node == NULL) {
							return_error = BR_FAILED_REPLY;
							goto err_binder_new_node_failed;
						}
						node->min_priority = fp->flags & FLAT_BINDER_FLAG_PRIORITY_MASK;
						node->accept_fds = !!(fp->flags & FLAT_BINDER_FLAG_ACCEPTS_FDS);
					}
					// 校验
					if (fp->cookie != node->cookie) {
					 // #
					}
					if (security_binder_transfer_binder(proc->tsk, target_proc->tsk)) {
						return_error = BR_FAILED_REPLY;
						goto err_binder_get_ref_for_node_failed;
					}
					// 在目标进程中为它创建引用，其实类似（在ServiceManager中创建bind_ref其实可以说，Servicemanager拥有全部Service的引用）
					ref = binder_get_ref_for_node(target_proc, node);
		 			// ￥
		 			// 修改flat_binder_object数据结构的type和handle域，接下来要传给接收方
					if (fp->type == BINDER_TYPE_BINDER)
						fp->type = BINDER_TYPE_HANDLE;
					else
						fp->type = BINDER_TYPE_WEAK_HANDLE;
					fp->handle = ref->desc;
					binder_inc_ref(ref, fp->type == BINDER_TYPE_HANDLE,
						       &thread->todo);
					trace_binder_transaction_node_to_ref(t, node, ref);
					binder_debug(BINDER_DEBUG_TRANSACTION,
						     "        node %d u%p -> ref %d desc %d\n",
						     node->debug_id, node->ptr, ref->debug_id,
						     ref->desc);
				} break;
				case BINDER_TYPE_HANDLE:
				case BINDER_TYPE_WEAK_HANDLE: {
				     // 通过引用号取得当前进程中对应的binder_ref结构体
					struct binder_ref *ref = binder_get_ref(proc, fp->handle);
			         // ￥
					if (security_binder_transfer_binder(proc->tsk, target_proc->tsk)) {
						return_error = BR_FAILED_REPLY;
						goto err_binder_get_ref_failed;
					}
					// 如果目标进程正好是提供该引用号对应的binder实体的进程，那么按照下面的方式
					修改flat_binder_object的相应域: type 和 binder，cookie。// 
					// 不太理解？？？
					if (ref->node->proc == target_proc) {
						if (fp->type == BINDER_TYPE_HANDLE)
							fp->type = BINDER_TYPE_BINDER;
						else
							fp->type = BINDER_TYPE_WEAK_BINDER;
						fp->binder = ref->node->ptr;
						fp->cookie = ref->node->cookie;
						binder_inc_node(ref->node, fp->type == BINDER_TYPE_BINDER, 0, NULL);
						trace_binder_transaction_ref_to_node(t, ref);
						 // $
					} else {
		 			// 否则会在目标进程的refs_by_node红黑树中先搜索看是否之前有创建过对应的b
		 			inder_ref，如果没有找到，那么就需要为ref->node节点在目标进程中新建一个目标进程的bi
		 			nder_ref挂入红黑树refs_by_node中。//
						struct binder_ref *new_ref;
						new_ref = binder_get_ref_for_node(target_proc, ref->node);
						if (new_ref == NULL) {
							return_error = BR_FAILED_REPLY;
							goto err_binder_get_ref_for_node_failed;
						}
						//此时只需要将此域修改为新建binder_ref的引用号
						fp->handle = new_ref->desc;
						binder_inc_ref(new_ref, fp->type == BINDER_TYPE_HANDLE, NULL);
						trace_binder_transaction_ref_to_ref(t, ref,
										    new_ref);
					}
				} break;
				
##### 如何唤醒目标进程或者线程 --直接唤醒等待在quene上的线程或者进程

		    // 这里如何获取 target->thread（如果有的化）， 目标进程或者线程，ref 对应node会有记录
				if (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {
					struct binder_transaction *tmp;
					// 目前肯定只有自己，因为阻塞，只能有自己
					tmp = thread->transaction_stack;
					 这里尝试解释一下 http://blog.csdn.net/universus/article/details/6211589 不知道是不是正确
					 关于工作线程的启动，Binder驱动还做了一点小小的优化。当进程P1的线程T1向进程P2发送请求时，
					 驱动会先查看一下线程T1是否也正在处理来自P2某个线程请求但尚未完成（没有发送回复）。这种情况
					  通常发生在两个进程都有Binder实体并互相对发时请求时。假如驱动在进程P2中发现了这样的线程，比如说T2
					 ，就会要求T2来处理T1的这次请求。因为T2既然向T1发送了请求尚未得到返回包，说明T2肯定（或将会）阻塞在读取返
					  回包的状态。这时候可以让T2顺便做点事情，总比等在那里闲着好。
					  而且如果T2不是线程池中的线程还可以为线程池分担部分工作，减少线程池使用率。//
		
					规则1：Client发给Server的请求数据包都提交到Server进程的全局to-do队列。不过有个特例，就
					是上节谈到的Binder对工作线程启动的优化。经过优化，来自T1的请求不是提交给P2的全局to-do队列，而是送入了T2
					的私有to-do队列。规则2：对同步请求的返回数据包（由BC_REPLY发送的包）都发送到发起请求的线程的私有to-do队列
					中。如上面的例子，如果进程P1的线程T1发给进程P2的线程T2的是同步请求，那么T2返回的数据包将送进T1的私有to-do队
					列而不会提交到P1的全局to-do队列。
					数据包进入接收队列的潜规则也就决定了线程进入等待队列的潜规则，
					即一个线程只要不接收返回数据包则应该在全局等待队列中等待新任务，
					否则就应该在其私有等待队列中等待Server的返回数据。还是上面的例子，
					T1在向T2发送同步请求后就必须等待在它私有等待队列中，
					而不是在P1的全局等待队列中排队，否则将得不到T2的返回的数据包。
		
					只有相互请求，才会在请求的时候发送到某个特定的线程。
					
					while (tmp) {
						if (tmp->from && tmp->from->proc == target_proc)
							target_thread = tmp->from;
						tmp = tmp->from_parent;
					}
				}
			}
			// 这里如果目标是线程，那么唤醒的就是线程的等待队列，如果是进程，就唤醒进程等待队列
			// 其实唤醒的时候，对应是queuen ，
			if (target_thread) {
				e->to_thread = target_thread->pid;
				target_list = &target_thread->todo;
				target_wait = &target_thread->wait;
			} else {
				target_list = &target_proc->todo;
				target_wait = &target_proc->wait;
			}
			e->to_proc = target_proc->pid;
			// TODO: reuse incoming transaction for reply 如何复用incoming transaction//
			t = kzalloc(sizeof(*t), GFP_KERNEL);
			//￥
			binder_stats_created(BINDER_STAT_TRANSACTION);
			//分配本次单边传输需要使用的binder_work结构体内存
			tcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);
			//￥
			binder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);
			t->debug_id = ++binder_last_id;
			e->debug_id = t->debug_id;
		    //￥ ;
		    //   reply 是不是回复 如果是同步传输的发送边，这里将当前的binder_thre
			if (!reply && !(tr->flags & TF_ONE_WAY))
			//  如果是同步传输的发送边，这里将当前的binder_thread记录
			// 在binder_transaction.from中，以供在同步传输的回复边时，驱动可以根据这个找到回复的目的task。
				t->from = thread;
			else
				// 如果是BC_REPLY或者是异步传输，这里不需要记录和返回信息相关的信息。//
				t->from = NULL;
		
			t->sender_euid = proc->tsk->cred->euid;
			//事务的目标进程  
			t->to_proc = target_proc;    
		    //事物的目标线程  
		    t->to_thread = target_thread;
			 // 这个保持不变，驱动不会关心它
			t->code = tr->code;
			t->flags = tr->flags;
			//线程的优先级的迁移
			t->priority = task_nice(current);
			trace_binder_transaction(reply, t, target_node);
			....
				if (reply) {
			BUG_ON(t->buffer->async_transaction != 0);
			// 这里是出栈操作
			binder_pop_transaction(target_thread, in_reply_to);
	
		} 
		// 如果是同步传输，请求方需要获取返回
		else if (!(t->flags & TF_ONE_WAY)) {
			BUG_ON(t->buffer->async_transaction != 0);
			// 需要带返回数据的标记
			t->need_reply = 1;
			// binder_transaction的的上一层任务，由于是堆栈，所以抽象为parent，惹歧义啊！这里是入栈操作
			t->from_parent = thread->transaction_stack;
			// 正在处理的事务栈，为何会有事务栈呢？因为在等待返回的过程中，还会有事务插入进去，比如null的reply
			thread->transaction_stack = t;
	  
		// 
	
		如果本次发起传输之前，当前task没有处于通讯过程中的话，这里必然为
		NULL。而且第一次发起传输时，这里也是为NULl。如果之前有异步传输没处理完，没处理完，就还没有release，
		那么这里不为null（为了自己，因为自己之前的任务还在执行，还没完），如果之前本task正在处理接收请求，这里也不为NULL(为了其他进程)，
	    这里将传输中间数据结构保存在binder_transaction链表顶部。这个transaction_stack实际
		上是管理这一个链表，只不过这个指针时时指向最新加入该链表的成员，最先加入的成员在最底部，有点类似于
		stack，所以这里取名叫transaction_stack。
	
		//
		} 
		// 异步传输，不需要返回
		else {
			// 不需要返回的情况下 ，下面是对异步通讯的分流处理
			BUG_ON(target_node == NULL);
			BUG_ON(t->buffer->async_transaction != 1);
			// 如果目标task仍然还有一个异步需要处理的话，该标志为1。//
			if (target_node->has_async_transaction) {
				// 将当前的这个的异步传输任务转入目标进程binder_node的异步等待队列async_todo中。为了加到异步中秋// 
				target_list = &target_node->async_todo;
				// 将后来的异步交互转入异步等待队列。就不唤醒了，因为有在执行的
				target_wait = NULL;
			} else
			// 如果没有，设置目标节点存在异步任务，设置也是在这里设置的，
				target_node->has_async_transaction = 1;
		}
		//
		t->work.type = BINDER_WORK_TRANSACTION;
		// 添加到目标的任务列表
		list_add_tail(&t->work.entry, target_list);
		tcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;
		// 添加到请求线程，也就是自己的待处理任务列表
		list_add_tail(&tcomplete->entry, &thread->todo);
		// 唤醒目标进程，查看是否需要唤醒，如果target_wait！=null
		// 当前task等待在task自己的等待队列中(binder_thread.todo)，永远只有其自己。//
		if (target_wait)
			wake_up_interruptible(target_wait);
		return;
	 	//err: ￥异常处理 。。。
	 
	 	
通过以上三步，binder驱动就会唤醒阻塞的ServiceManager，ServiceManager会解析、处理addService请求：在前面分析ServiceManager成为关键的时候，ServiceManager阻塞在binder_loop(bs, svcmgr_handler)中，被binder驱动唤醒后继续运行：

	int svcmgr_handler(struct binder_state *bs, struct binder_txn *txn, struct binder_io *msg, struct binder_io *reply) { struct svcinfo *si; uint16_t *s; unsigned len; void *ptr; uint32_t strict_policy;
	
 	
	if (txn->target != svcmgr_handle)

	strict_policy = bio_get_uint32(msg);
	s = bio_get_string16(msg, &len);
	if ((len != (sizeof(svcmgr_id) / 2)) ||
	    memcmp(svcmgr_id, s, sizeof(svcmgr_id))) {
	    fprintf(stderr,"invalid id %s\n", str8(s));
	    return -1;
	}
	
	switch(txn->code) {
	case SVC_MGR_GET_SERVICE:
	case SVC_MGR_CHECK_SERVICE:
	    s = bio_get_string16(msg, &len);
	    ptr = do_find_service(bs, s, len);
	    if (!ptr)
	        break;
	    bio_put_ref(reply, ptr);
	    return 0;
	case SVC_MGR_ADD_SERVICE:
	    s = bio_get_string16(msg, &len);
	    ptr = bio_get_ref(msg);
	    if (do_add_service(bs, s, len, ptr, txn->sender_euid))
	        return -1;
	    break;
	
	case SVC_MGR_LIST_SERVICES: {
	    unsigned n = bio_get_uint32(msg);
	
	    si = svclist;
	    while ((n-- > 0) && si)
	        si = si->next;
	    if (si) {
	        bio_put_string16(reply, si->name);
	        return 0;
	    }
	    return -1;
	}
	default:
	    LOGE("unknown code %d\n", txn->code);
	    return -1;
	}
	
	bio_put_uint32(reply, 0);
	return 0; }
	
可以看到，addService最终会调用do_add_service函数:

	int do_add_service(struct binder_state *bs,
	                   uint16_t *s, unsigned len,
	                   void *ptr, unsigned uid, int allow_isolated)
	{
	    struct svcinfo *si;
 	
	    if (!ptr || (len == 0) || (len > 127))
	        return -1;
	
	    if (!svc_can_register(uid, s)) {
	        return -1;
	    }
	
	    si = find_svc(s, len);
	    if (si) {
	        if (si->ptr) {
	            svcinfo_death(bs, si);
	        }
	        si->ptr = ptr;
	    } else {
	        si = malloc(sizeof(*si) + (len + 1) * sizeof(uint16_t));
	        if (!si) {
	            ALOGE("add_service('%s',%p) uid=%d - OUT OF MEMORY\n",
	                 str8(s), ptr, uid);
	            return -1;
	        }
	        si->ptr = ptr;
	        si->len = len;
	        memcpy(si->name, s, (len + 1) * sizeof(uint16_t));
	        si->name[len] = '\0';
	        si->death.func = svcinfo_death;
	        si->death.ptr = si;
	        si->allow_isolated = allow_isolated;
	        si->next = svclist;
	        svclist = si;
	    }
	
	    binder_acquire(bs, ptr);
	    binder_link_to_death(bs, ptr, &si->death);
	    return 0;
	}

这样就将Service添加到ServiceManager的svclist列表中去了，只有，Client就可以同getService去ServiceManager查找相应Service了。

<a name="service_self_implement"></a>

#### Server如何开启自身的Binder监听Loop

Native层，Binder实体一般都要实现BBinder接口，关键是实现onTransact函数，这是Android对于Binder通信框架的抽象。刚才在Add的时候，只谈到了BpServiceManager，没说Server的MediaPlayerService实体，其实MediaPlayerService继承自BnMediaPlayerServiceclass， BnMediaPlayerService又继承与BnInterface<IMediaPlayerService>，这里就是Bn与Bp对于Binder框架最好的描述，两者利用泛型与继承的结合，在上层很好的用依赖倒置的方式实现了对框架的概括，需要实现什么业务逻辑，只需要定义一份接口，然后让Bn与Bp实现即可。Anroid的Binder的框架如何抽象非常完美，业务逻辑与底层通信实现了完美的分离与统一，既兼顾了通信的统一与考虑了应用的扩展。业务逻辑的实现是对应于BBinder实体，但是循环监听的开启对应于进程与线程，一个Native进程，只需要调用以下函数即可让自己进入Binder监听Loop ，其核心函数还是talkWithDriver：

	  		 ProcessState::self()->startThreadPool();
	   		 IPCThreadState::self()->joinThreadPool();

其实只需要最上面一行就够了，IPCThreadState::self()->joinThreadPool();linux中如果主线程如果退出，程序就退出，所以主线程必须要进入循环，但是与其让他退出，不如重复利用，于是两个Binder线程中，一个是专门开启的，一个是赠送的，核心的binder驱动交互还是要走到talkWithDriver
	    
      void IPCThreadState::joinThreadPool(bool isMain){
      
		    mOut.writeInt32(isMain ? BC_ENTER_LOOPER : BC_REGISTER_LOOPER);    
		    status_t result;
      			 do {
	            ...	            		
	 		        result = talkWithDriver();
			        if (result >= NO_ERROR) {
			            size_t IN = mIn.dataAvail();
			            if (IN < sizeof(int32_t)) continue;
			            cmd = mIn.readInt32();
			            result = executeCommand(cmd);
			        }  
		        
		   
		    } while (result != -ECONNREFUSED && result != -EBADF);
		
	 
		        (void*)pthread_self(), getpid(), (void*)result);
		    
		    mOut.writeInt32(BC_EXIT_LOOPER);
		    talkWithDriver(false);
		};	
			


如果是Java层App，所有的进程都是Zygote的子进程，不用担心因为ActivityThread会保证UI Thread进入循环。startThreadPool回传给底层BC_ENTER_LOOPER参数，其目的是什么，只是向Binder驱动注册通报一声，我这个服务要作为Binder监听主线程服务了，因为有些Client在请求Server的时候，Server可能还没进入Loop，当前进程中实现了BBinder服务，并且完成了注册，那么ProcessState::self()->startThreadPool();开启的线程就可以访问改BBinder，这个底层是统一的。只要进入循环，Binder就起来了，起来两个线程，那么最终请求端会唤醒哪个线程呢？后面讨论，到这里，其实Service的Loop就算
 起来了。

<a nclient_part_arch_arch"></a>

#### Client请求的实现逻辑

<a name="client_get_proxy"></a>

##### Client请求ServiceManager获得Service代理

Client请求是一般是在当前线程中，看下MediaPlayService的用法，在java代码中，MediaPlayService的体现是MediaPlayer这个类，而开发者的用法一般如下

	MediaPlayer mMediaPlayer = new MediaPlayer(); 
	mMediaPlayer.setDataSource(path); 
	mMediaPlayer.prepareAsync();

Java是解释类语言，其具体实现要去Native代码中去查找
	
	public MediaPlayer/* {
	// Native setup requires a weak reference to our object. * It’s easier to create it here than in C++. */ 
	native_setup(new WeakReference(this));//关键是这句，JNI的使用 }

native_setup函数在android_media_MediaPlayer.cpp中实现实现代码如下：

	static void android_media_MediaPlayer_native_setup(JNIEnv *env, jobject thiz, jobject weak_this) {
	 sp mp = new MediaPlayer(); //创建于Java层对应的c++层MediaPlayer实例，并设置给Java 
	 if (mp == NULL) { 
	 jniThrowException(env, "java/lang/RuntimeException", "Out of memory"); 
	 return; 
	 } 
	 // create new listener and give it to MediaPlayer 
	 sp listener = new JNIMediaPlayerListener(env, thiz, weak_this); 
	 mp->setListener(listener); 
	 
	 setMediaPlayer(env, thiz, mp); 
	 }

以上代码的主要功能是，创建Java与C++之间的的相互引用，类似于实现C++的对象封装，并没有与Binder的进行交互。继续看MediaPlayer的使用setDataSource，

	status_t MediaPlayer::setDataSource(const sp<IStreamSource> &source)
	{
	    ALOGV("setDataSource");
	    status_t err = UNKNOWN_ERROR;
	    const sp<IMediaPlayerService>& service(getMediaPlayerService());
	    if (service != 0) {
	        sp<IMediaPlayer> player(service->create(this, mAudioSessionId));
	        if ((NO_ERROR != doSetRetransmitEndpoint(player)) ||
	            (NO_ERROR != player->setDataSource(source))) {
	            player.clear();
	        }
	        err = attachNewPlayer(player);
	    }
	    return err;
	}

这部分代码的主要功能是，创建MediaPlayer代理，其实就是变相的创建BpMediaPlayerService。getMediaPlayerService这里是获取的Hander引用, getMediaPlayerService在IMediaDeathNotifier.cpp中。

	
	// establish binder interface to MediaPlayerSer/*ce
	//static*/const sp<IMediaPlayerService>&
	IMediaDeathNotifier::getMediaPlayerService()
	{
	    ALOGV("getMediaPlayerService");
	    Mutex::Autolock _l(sServiceLock);
	    if (sMediaPlayerService == 0) {
	        sp<IServiceManager> sm = defaultServiceManager();
	        sp<IBinder> binder;
	        do {
	            binder = sm->getService(String16("media.player")); //这里开始获取MediaPlayer服务
	            if (binder != 0) {
	                break;
	            }
	            ALOGW("Media player service not published, waiting...");
	            usleep(500000); // 0.5 s
	        } while (true);
	
	        if (sDeathNotifier == NULL) {
	        sDeathNotifier = new DeathNotifier();
	    }
	    binder->linkToDeath(sDeathNotifier);
	    sMediaPlayerService = interface_cast<IMediaPlayerService>(binder);
	    }
	    return sMediaPlayerService;
	}

这里终于涉及到Binder部分，比较重要的三句话:

* defaultServiceManager()获取SVM
* getService查询并获取Service
* interface_cast 创建Service本地代理

如此，Client去获取Service代理的路就通了。首先看第一部分defaultServiceManager(),这部分代码位于IServiceManager.cpp中,之前addService已做过分析，这里再细化一下：

	sp<IServiceManager> defaultServiceManager()
	{
	    if (gDefaultServiceManager != NULL) return gDefaultServiceManager;
	    
	    {
	        AutoMutex _l(gDefaultServiceManagerLock);
	        if (gDefaultServiceManager == NULL) {
	            gDefaultServiceManager = interface_cast<IServiceManager>(
	                ProcessState::self()->getContextObject(NULL));
	        }
	    }
	    
	    return gDefaultServiceManager; }
顺着这条单路走下去，最后会发现，上面的代码可以转化为

	gDefaultServiceManager = interface_cast<IServiceManager>(new BpBinder(0));
				android::sp<IServiceManager> IServiceManager::asInterface(const android::sp<android::IBinder>& obj)
				{
				    android::sp<IServiceManager> intr;
				    if (obj != NULL) {
				        intr = static_cast<IServiceManager*>(
				            obj->queryLocalInterface(
				                    IServiceManager::descriptor).get());
				        if (intr == NULL) {
				            intr = new BpServiceManager(obj);
				        }
				    }
				    return intr;
				}
BpServiceManager是一个BpRef对象，因为内部必有一个对象引用BpBinder，果不其然，mRemote就是这个引用。BpServiceManager其实自己的代码更加倾向于抽象ServiceManager服务接口，而BpBinder对客户单而言，是真正用于Binder进程通信的工具，BpBinder同业务逻辑没有任何关系，比如Mediaplayer的 play，stop与BpBinder完全不关系，BpBinder它只负责将请求传输到Binder驱动，这里就可以看出业务跟通信的分离BpServiceManager(BpBider)这里第一部分就完结了。

第二部分，目标Service远程代理的的获取，代码如下：

			    virtual sp<IBinder> getService(const String16& name) const
			    {
			        unsigned n;
			        for (n = 0; n < 5; n++){
			            sp<IBinder> svc = checkService(name);
			            if (svc != NULL) return svc;
			            
			            sleep(1);
			        }
			        return NULL;
			    }
			    virtual sp<IBinder> checkService( const String16& name) const
			    {
			        Parcel data, reply;
			        data.writeInterfaceToken(IServiceManager::getInterfaceDescriptor());
			        data.writeString16(name);
			        remote()->transact(CHECK_SERVICE_TRANSACTION, data, &reply);
			        return reply.readStrongBinder();
			    }
可以看出，最终会调用checkService去向SVM发送请求，最终返回IBinder类，作为Client和目标对象通信的接口。remote()返回的其实是BpBinder的引用，remote()->transact(CHECK_SERVICE_TRANSACTION, data, &reply)最终调用的是BpBinder的transact函数：

	status_t BpBinder::transact(
	    uint32_t code, const Parcel& data, Parcel* reply, uint32_t flags)
	{
	    // Once a Binder has died, it will never come back to life.
	    if (mAlive) {
	        status_t status = IPCThreadState::self()->transact(
	            mHandle, code, data, reply, flags);
	        if (status == DEAD_OBJECT) mAlive = 0;
	        return status;
	    }
	
	    return DEAD_OBJECT;
	}
IPCThreadState最终调用那个自己的transact函数，将请求传递给Binder驱动，transact关键代码如下：

	status_t IPCThreadState::transact(int32_t handle,
	                                  uint32_t code, const Parcel& data,
	                                  Parcel* reply, uint32_t flags){
	    status_t err = data.errorCheck();
	    flags |= TF_ACCEPT_FDS;
	    if (err == NO_ERROR) {
	        err = writeTransactionData(BC_TRANSACTION, flags, handle, code, data, NULL);
	    }
	    if ((flags & TF_ONE_WAY) == 0) {
	。。。
	        if (reply) {
	            err = waitForResponse(reply);
	        } else {
	            Parcel fakeReply;
	            err = waitForResponse(&fakeReply);
	        }
IPCThreadState首先调用那个writeTransactionData整合数据，注意，只是整合数据，之后调用waitForResponse向SVM发送数据，

	status_t IPCThreadState::writeTransactionData(int32_t cmd, uint32_t BinderFlags,
	    int32_t handle, uint32_t code, const Parcel& data, status_t* statusBuffer)
	{
	    Binder_transaction_data tr;
	
	    tr.target.handle = handle;
	    tr.code = code;
	    tr.flags = BinderFlags;
		...
	        tr.data_size = data.ipcDataSize();
	        tr.data.ptr.buffer = data.ipcData();
	        tr.offsets_size = data.ipcObjectsCount()*sizeof(size_t);
	        tr.data.ptr.offsets = data.ipcObjects();
		...	    
	    mOut.writeInt32(cmd);
	    mOut.write(&tr, sizeof(tr));
	    
	    return NO_ERROR;
	}
	    
上面的代码并没有直接与Binder交互，只是整合数据，waitForResponse才会开始交互

	status_t IPCThreadState::waitForResponse(Parcel *reply, status_t *acquireResult) { int32_t cmd; int32_t err;
	
	while (1) {
	//写请求 if ((err=talkWithDriver()) < NO_ERROR) break;
	
	//mIn 这是已经写入了数据，下面就是根据返回去处理 
	
	err = mIn.errorCheck(); 
	if (err < NO_ERROR) break;
	 if (mIn.dataAvail() == 0) continue;
	
	    cmd = mIn.readInt32();
 
	    switch (cmd) {
	    case BR_TRANSACTION_COMPLETE:
	        if (!reply && !acquireResult) goto finish;
	        break;
	    
	    case BR_DEAD_REPLY:
	        err = DEAD_OBJECT;
	        goto finish;
			...
	    
	    case BR_REPLY:
	        {
	            Binder_transaction_data tr;
	            err = mIn.read(&tr, sizeof(tr));
	            LOG_ASSERT(err == NO_ERROR, "Not enough command data for brREPLY");
	            if (err != NO_ERROR) goto finish;
	
	            if (reply) {
	                if ((tr.flags & TF_STATUS_CODE) == 0) {
	                    reply->ipcSetDataReference(
	                        reinterpret_cast<const uint8_t*>(tr.data.ptr.buffer),
	                        tr.data_size,
	                        reinterpret_cast<const size_t*>(tr.data.ptr.offsets),
	                        tr.offsets_size/sizeof(size_t),
	                        freeBuffer, this);
	                } else {
	                    err = *static_cast<const status_t*>(tr.data.ptr.buffer);
	                    freeBuffer(NULL,
	                        reinterpret_cast<const uint8_t*>(tr.data.ptr.buffer),
	                        tr.data_size,
	                        reinterpret_cast<const size_t*>(tr.data.ptr.offsets),
	                        tr.offsets_size/sizeof(size_t), this);
	                }
	            } ...
	        }
	        goto finish;
	
	    default:
	        err = executeCommand(cmd);
	        if (err != NO_ERROR) goto finish;
		...

注意While（1）的次数与作用，在waitForResponse的作用：一直等到有意义的反馈，才会停止等待，这里其实又多次握手的。
			
	while (1) {
	   if ((err=talkWithDriver()) < NO_ERROR) break;
	   err = mIn.errorCheck();
	   if (err < NO_ERROR) break;
	   if (mIn.dataAvail() == 0) continue;
	 talkWithDriver死循环保证数据正确的传递到内核：
	do {
	 #if defined(HAVE_ANDROID_OS)
	   if (ioctl(mProcess->mDriverFD, BINDER_WRITE_READ, &bwr) >= 0)
	       err = NO_ERROR;
	   else
	       err = -errno; #else
	   err = INVALID_OPERATION; #endif
	   IF_LOG_COMMANDS() {
	       alog << "Finished read/write, write size = " << mOut.dataSize() << endl;
	   }
	} while (err == -EINTR);

ioctrl 写命令，写返回，读取返回，为什么先要写返回，这里类似tcp协议，告诉已发送，在让其进入等待，分阶段来处理命令的发送。

	static long Binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg) { int ret; struct Binder_proc *proc = filp->private_data; struct Binder_thread *thread; unsigned int size = _IOC_SIZE(cmd);
	
	switch (cmd) {
	case BINDER_WRITE_READ: {
		struct Binder_write_read bwr;
		if (size != sizeof(struct Binder_write_read)) {
			ret = -EINVAL;
			goto err;
		}
		if (copy_from_user(&bwr, ubuf, sizeof(bwr))) {
			ret = -EFAULT;
			goto err;
		} ….
		if (bwr.write_size > 0) {
			ret = Binder_thread_write(proc, thread, (void __user *)bwr.write_buffer, bwr.write_size, &bwr.write_consumed);
			trace_Binder_write_done(ret);
			if (ret < 0) {
				bwr.read_consumed = 0;
				if (copy_to_user(ubuf, &bwr, sizeof(bwr)))
					ret = -EFAULT;
				goto err;
			}
		}
		              …..
		if (bwr.read_size > 0) {
			ret = Binder_thread_read(proc, thread, (void __user *)bwr.read_buffer, bwr.read_size, &bwr.read_consumed, filp->f_flags & O_NONBLOCK);
			trace_Binder_read_done(ret);
			if (!list_empty(&proc->todo))
				wake_up_interruptible(&proc->wait);
			if (ret < 0) {
				if (copy_to_user(ubuf, &bwr, sizeof(bwr)))
					ret = -EFAULT;
				goto err;
			}
		}
	             …..
	
		if (copy_to_user(ubuf, &bwr, sizeof(bwr))) {
			ret = -EFAULT;
			goto err;
		}
		break;
	          else
			Binder_context_mgr_uid = current->cred->euid;
			
		Binder_context_mgr_node = Binder_new_node(proc, NULL, NULL);
		if (Binder_context_mgr_node == NULL) {
			ret = -ENOMEM;
			goto err;
		}
	 
			goto err;
		}	 
	 
	if (thread)
		thread->looper &= ~BINDER_LOOPER_STATE_NEED_RETURN;
	Binder_unlock(__func__);
	wait_event_interruptible(Binder_user_error_wait, Binder_stop_on_user_error < 2);
	if (ret && ret != -ERESTARTSYS)
		printk(KERN_INFO "Binder: %d:%d ioctl %x %lx returned %d\n", proc->pid, current->pid, cmd, arg, ret); err_unlocked:
	trace_Binder_ioctl_done(ret);
	return ret; }
	
ioctrl 写命令，写返回，读取返回，为什么先要写返回，这里类似tcp协议，告诉已发送，在让其进入等待，分阶段来处理命令的发送。这里还要注意，比较重要的是下面的Smgr的写返回处理，在那里，要为客户端添加bind_node的ref，并且在客户端中创建引用句柄，注意，注意，引用句柄只是对客户端 有效，客户端用句柄在客户端查找出Server的bind_proc，然后处理。 Android框架层已经应用层天生支持Binder通信 有了这个线程池之后，我们在开发Android应用程序的时候，当我们要和其它进程中进行通信时，只要定义自己的Binder对象，然后把这个Binder对象的远程接口通过其它途径传给其它进程后，其它进程就可以通过这个Binder对象的远程接口来调用我们的应用程序进程的函数了，它不像我们在C++层实现Binder进程间通信机制的Server时，必须要手动调用IPCThreadState.joinThreadPool函数来进入一个无限循环中与Binder驱动程序交互以便获得Client端的请求，这样就实现了我们

在文章开头处说的Android应用程序进程天然地支持Binder进程间通信机制。


<a name="client_service"></a>

#####  Client请求Service服务

##### 将请求数据发送给目标Service 
##### 等待返回

 
 