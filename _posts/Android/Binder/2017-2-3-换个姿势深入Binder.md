---
layout: post
title: "换个姿势深入Binder"
categories: [Android]
image: http://upload-images.jianshu.io/upload_images/1460468-506ac9b7dffe1f33.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240


---

# 换个姿势深入Binder通信

对于每个做Android开发的人来说，或多或少都知道Binder在Android系统中的重要程度，它承担了绝大部分Android进程通信的职责，可以看做是Android的血管系统，负责不同服务模块进程间的通信。在对Binder的理解上，可大可小，在日常APP开发并不怎么涉及Binder通信知识，最多就是Service及AIDL的使用会涉及部分Binder知识。往少了说，Binder就是一句话：Binder是一种IPC进程间通信方式，负责进程A的数据，发送到进程B。不过如果往大了说，其实涉及的知识还是很多的，如Binder驱动、Android中对于Binder驱动的扩展、Zygote进程孵化中对于Binder通信的支持、Java层Binder封装，Native层Binder通信的封装等等，Binder讣告机制等等。网上很多分析Binder框架的文章，一般都是从ServiceManager、Binder驱动、addService、getService来分析等来分析，其实这些地方主要是针对系统提供的服务，跟我们常见的bindService还是有一定区别的，这些文章一般分析的都很详细，而且篇幅都很大，缺点就是看的太多，整体下来抓不住一些关键的点，看完了，当时能理解个大概，过段时间，基本忘干净了。本篇文章以读者有一定Binder基础为前提，并且对于每个问题，不会太细的跟踪分析，只抛砖，读者自己去发掘玉：

*  Binder的定向制导，如何找到目标Binder，唤起进程或者线程
*  Binder中的红黑树，为什么会有两棵binder_ref红黑树
*  Binder一次拷贝原理(直接拷贝到目标线程的内核空间，内核空间与用户空间对应)
*  Binder传输数据的大小限制（内核4M 上层限制1m-8k），传输Bitmap过大，就会崩溃的原因，Activity之间传输BitMap
*  系统服务与bindService等启动的服务的区别
*  Binder线程、Binder主线程、Client请求线程的概念与区别
*  Binder请求Client是同步而Server是异步到底说的什么
*  Android APP进程天生支持Binder通信的原理是什么 
*  Android APP有多少Binder线程，是固定的么

*  Binder线程唤醒与进程唤醒（服务一般是睡眠在进程等待队列，Client一般睡眠在自己的线程等待队列上）
*  Binder不同层次使用的数据结构（命令的封装）
*  BC与BR的区别

*  bindservice的流程（流程 双Binder通信机制，互为CS）
*  binder通信双通道的理解（流程 双Binder通信机制，互为CS
*  Binder实体与Handler的转换
*  Binder驱动中数据缓存的释放（释放时机，上层的通知）

*  Java层BinderProxy的实例化（通过Parcel的read）
*  Parcel read与write Binder的原理

*  Binder线程的动态增加
*  Zygote进程孵化的子进程为何天然支持binder通信


*  同一个binder_trasaction位于两个栈上
*  binder_work私有唤起队列
*  binder中数据结构的传递，与数据的传递的区分
*  不同层面Binder通信用的数据结构封装
*  Java层Binder与Native层的转化对应
*  Binder服务线程同请求线程的区别
*  画一个基于Binder的Client与Server模型，线程与实体分开
*  Binder实体onTransaction的回调入口
*  数据传递参数AIDL 单向与双向 （）



## Binder如何精确制导，找到目标Binder实体，并唤醒进程或者线程

Binder实体服务其实有两种，一是通过addService注册到ServiceManager中的服务，比如ActvityManagerService、PackageManagerService、PowerManagerService等，一般都是系统服务；还有一种是通过bindServcice拉起的一些服务，一般是开发者自己实现的服务。这里先看通过addService添加的被ServiceManager所管理的服务。有很多分析ServiceManager的文章，本文不分析ServiceManager，只是简单提一下，ServiceManager是比较特殊的服务，所有应用都能直接使用，因为ServiceManager对于Client端来说Handle句柄是固定的，都是0，所以ServiceManager服务并不需要查询，可以直接使用。

理解Binder定向制导的关键是理解Binder的四棵红黑树，先看一下binder_proc结构体，在它内部有四棵红黑树，threads，nodes，refs_by_desc，refs_by_node，nodes就是Binder实体在内核中对应的数据结构，binder_node里记录进程相关的binder_proc，还有Binder实体自身的地址等信息，nodes红黑树位于binder_proc，可以知道Binder实体其实是进程内可见，而不是线程内。

	struct binder_proc {
		struct hlist_node proc_node;
		struct rb_root threads;
		struct rb_root nodes;
		struct rb_root refs_by_desc;
		struct rb_root refs_by_node;
		。。。
		struct list_head todo;
		wait_queue_head_t wait;
		。。。
	};
	
现在假设存在一堆Client与Service，Client如何才能访问Service呢？首先Service会通过addService将binder实体注册到ServiceManager中去，Client如果想要使用Servcie，就需要通过getService向ServiceManager请求该服务。在Service通过addService向ServiceManager注册的时候，ServiceManager会将服务相关的信息存储到自己进程的Service列表中去，同时在ServiceManager进程的binder_ref红黑树中为Service添加binder_ref节点，这样ServiceManager就能获取Service的Binder实体信息。而当Client通过getService向ServiceManager请求该Service服务的时候，ServiceManager会在注册的Service列表中查找该服务，如果找到就将该服务返回给Client，在这个过程中，ServiceManager会在Client进程的binder_ref红黑树中添加binder_ref节点，可见**本进程中的binder_ref红黑树节点都不是本进程自己创建的，要么是Service进程将binder_ref插入到ServiceManager中去，要么是ServiceManager进程将binder_ref插入到Client中去**。之后，Client就能通过Handle句柄获取binder_ref，进而访问Service服务。

![binder_ref添加逻辑](http://upload-images.jianshu.io/upload_images/1460468-6cec53bfbbdc0aa1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

getService之后，便可以获取binder_ref引用，进而获取到binder_proc与binder_node信息，之后Client便可有目的的将binder_transaction事务插入到binder_proc的待处理列表，并且，如果进程正在睡眠，就唤起进程，其实这里到底是唤起进程还是线程也有讲究，对于Client向Service发送请求的状况，一般都是唤醒binder_proc上睡眠的线程：

	struct binder_ref {
		int debug_id;
		struct rb_node rb_node_desc;
		struct rb_node rb_node_node;
		struct hlist_node node_entry;
		struct binder_proc *proc;
		struct binder_node *node;
		uint32_t desc;
		int strong;
		int weak;
		struct binder_ref_death *death;
	};
 

##  binder_proc为何会有两棵binder_ref红黑树
 
binder_proc中存在两棵binder_ref红黑树，其实两棵红黑树中的节点是复用的，只是查询方式不同，一个通过handle句柄，一个通过node节点查找。个人理解：refs_by_node红黑树主要是为了
binder驱动往用户空间写数据所使用的，而refs_by_desc是用户空间向Binder驱动写数据使用的，只是方向问题。比如在服务addService的时候，binder驱动会在在ServiceManager进程的binder_proc中查找binder_ref结构体，如果没有就会新建binder_ref结构体，再比如在Client端getService的时候，binder驱动会在Client进程中通过 binder_get_ref_for_node为Client创建binder_ref结构体，并分配句柄，同时插入到refs_by_desc红黑树中，可见refs_by_node红黑树，主要是给binder驱动往用户空间写数据使用的。相对的refs_by_desc主要是为了用户空间往binder驱动写数据使用的，当用户空间已经获得Binder驱动为其创建的binder_ref引用句柄后，就可以通过binder_get_ref从refs_by_desc找到响应binder_ref，进而找到目标binder_node。可见有两棵红黑树主要是区分使用对象及数据流动方向，看下面的代码就能理解：

	// 根据32位的uint32_t desc来查找，可以看到，binder_get_ref不会新建binder_ref节点
	static struct binder_ref *binder_get_ref(struct binder_proc *proc,
						 uint32_t desc)
	{
		struct rb_node *n = proc->refs_by_desc.rb_node;
		struct binder_ref *ref;
		while (n) {
			ref = rb_entry(n, struct binder_ref, rb_node_desc);
			if (desc < ref->desc)
				n = n->rb_left;
			else if (desc > ref->desc)
				n = n->rb_right;
			else
				return ref;
		}
		return NULL;
	}

可以看到binder_get_ref并具备binder_ref的创建功能，相对应的看一下binder_get_ref_for_node，binder_get_ref_for_node红黑树主要通过binder_node进行查找，如果找不到，就新建binder_ref，同时插入到两棵红黑树中去

	static struct binder_ref *binder_get_ref_for_node(struct binder_proc *proc,
							  struct binder_node *node)
	{
		struct rb_node *n;
		struct rb_node **p = &proc->refs_by_node.rb_node;
		struct rb_node *parent = NULL;
		struct binder_ref *ref, *new_ref;
		while (*p) {
			parent = *p;
			ref = rb_entry(parent, struct binder_ref, rb_node_node);
			if (node < ref->node)
				p = &(*p)->rb_left;
			else if (node > ref->node)
				p = &(*p)->rb_right;
			else
				return ref;
		}
	
		// binder_ref 可以在两棵树里面，但是，两棵树的查询方式不同，并且通过desc查询，不具备新建功能
		new_ref = kzalloc(sizeof(*ref), GFP_KERNEL);
		if (new_ref == NULL)
			return NULL;
		binder_stats_created(BINDER_STAT_REF);
		new_ref->debug_id = ++binder_last_id;
		new_ref->proc = proc;
		new_ref->node = node;
		rb_link_node(&new_ref->rb_node_node, parent, p);
		// 插入到proc->refs_by_node红黑树中去
		rb_insert_color(&new_ref->rb_node_node, &proc->refs_by_node);
		// 是不是ServiceManager的
		new_ref->desc = (node == binder_context_mgr_node) ? 0 : 1;
		// 分配Handle句柄，为了插入到refs_by_desc
		for (n = rb_first(&proc->refs_by_desc); n != NULL; n = rb_next(n)) {
			ref = rb_entry(n, struct binder_ref, rb_node_desc);
			if (ref->desc > new_ref->desc)
				break;
			new_ref->desc = ref->desc + 1;
		}
		// 找到目标位置
		p = &proc->refs_by_desc.rb_node;
		while (*p) {
			parent = *p;
			ref = rb_entry(parent, struct binder_ref, rb_node_desc);
			if (new_ref->desc < ref->desc)
				p = &(*p)->rb_left;
			else if (new_ref->desc > ref->desc)
				p = &(*p)->rb_right;
			else
				BUG();
		}
		rb_link_node(&new_ref->rb_node_desc, parent, p);
		// 插入到refs_by_desc红黑树中区
		rb_insert_color(&new_ref->rb_node_desc, &proc->refs_by_desc);
	
		if (node) {
			hlist_add_head(&new_ref->node_entry, &node->refs);
			binder_debug(BINDER_DEBUG_INTERNAL_REFS,
				     "binder: %d new ref %d desc %d for "
				     "node %d\n", proc->pid, new_ref->debug_id,
				     new_ref->desc, node->debug_id);
		} else {
			binder_debug(BINDER_DEBUG_INTERNAL_REFS,
				     "binder: %d new ref %d desc %d for "
				     "dead node\n", proc->pid, new_ref->debug_id,
				      new_ref->desc);
		}
		return new_ref;
	}
	
该函数调用在binder_transaction函数中，其实就是在binder驱动访问target_proc的时候，这也也很容易理解，Handle句柄对于跨进程没有任何意义，进程A中的Handle，放到进程B中是无效的。

![两棵binder_ref红黑树](http://upload-images.jianshu.io/upload_images/1460468-2a2331213527a259.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## Binder一次拷贝原理

Android选择Binder作为主要进程通信的方式同其性能高也有关系，Binder只需要一次拷贝就能将A进程用户空间的数据为B进程所用。这里主要涉及两个点：

* Binder的map函数，会将内核空间直接与用户空间对应，用户空间可以直接访问内核空间的数据
* A进程的数据会被直接拷贝到B进程的内核空间（一次拷贝）
	
		#define BINDER_VM_SIZE ((1*1024*1024) - (4096 *2))
		
		ProcessState::ProcessState()
		    : mDriverFD(open_driver())
		    , mVMStart(MAP_FAILED)
		    , mManagesContexts(false)
		    , mBinderContextCheckFunc(NULL)
		    , mBinderContextUserData(NULL)
		    , mThreadPoolStarted(false)
		    , mThreadPoolSeq(1){
		   if (mDriverFD >= 0) {
			....
		        // mmap the binder, providing a chunk of virtual address space to receive transactions.
		        mVMStart = mmap(0, BINDER_VM_SIZE, PROT_READ, MAP_PRIVATE | MAP_NORESERVE, mDriverFD, 0);
		   ...
		 
		    }
		}
mmap函数属于系统调用，mmap会从当前进程中获取用户态可用的虚拟地址空间（vm_area_struct *vma），并在mmap_region中真正获取vma，然后调用file->f_op->mmap(file, vma)，进入驱动处理，之后就会在内存中分配一块连续的虚拟地址空间，并预先分配好页表、已使用的与未使用的标识、初始地址、与用户空间的偏移等等，通过这一步之后，就能把Binder在内核空间的数据直接通过指针地址映射到用户空间，供进程在用户空间使用，这是一次拷贝的基础，一次拷贝在内核中的标识如下：

		struct binder_proc {
		struct hlist_node proc_node;
		// 四棵比较重要的树 
		struct rb_root threads;
		struct rb_root nodes;
		struct rb_root refs_by_desc;
		struct rb_root refs_by_node;
		int pid;
		struct vm_area_struct *vma; //虚拟地址空间，用户控件传过来
		struct mm_struct *vma_vm_mm;
		struct task_struct *tsk;
		struct files_struct *files;
		struct hlist_node deferred_work_node;
		int deferred_work;
		void *buffer; //初始地址
		ptrdiff_t user_buffer_offset; //这里是偏移
		
		struct list_head buffers;//这个列表连接所有的内存块，以地址的大小为顺序，各内存块首尾相连
		struct rb_root free_buffers;//连接所有的已建立映射的虚拟内存块，以内存的大小为index组织在以该节点为根的红黑树下
		struct rb_root allocated_buffers;//连接所有已经分配的虚拟内存块，以内存块的开始地址为index组织在以该节点为根的红黑树下
		
		}

上面只是在APP启动的时候开启的地址映射，但并未涉及到数据的拷贝，下面看数据的拷贝操作。**当数据从用户空间拷贝到内核空间的时候，是直从当前进程的用户空间接拷贝到目标进程的内核空间，这个过程是在请求端线程中处理的，操作对象是目标进程的内核空间**。看如下代码：

	static void binder_transaction(struct binder_proc *proc,
				       struct binder_thread *thread,
				       struct binder_transaction_data *tr, int reply){
				       ...
			在通过进行binder事物的传递时，如果一个binder事物（用struct binder_transaction结构体表示）需要使用到内存，
			就会调用binder_alloc_buf函数分配此次binder事物需要的内存空间。
			需要注意的是：这里是从目标进程的binder内存空间分配所需的内存
			//从target进程的binder内存空间分配所需的内存大小,这也是一次拷贝，完成通信的关键，直接拷贝到目标进程的内核空间
			//由于用户空间跟内核空间仅仅存在一个偏移地址，所以也算拷贝到用户空间
			t->buffer = binder_alloc_buf(target_proc, tr->data_size,
				tr->offsets_size, !reply && (t->flags & TF_ONE_WAY));
			t->buffer->allow_user_free = 0;
			t->buffer->debug_id = t->debug_id;
			//该binder_buffer对应的事务    
			t->buffer->transaction = t;
			//该事物对应的目标binder实体 ,因为目标进程中可能不仅仅有一个Binder实体
			t->buffer->target_node = target_node;
			trace_binder_transaction_alloc_buf(t->buffer);
			if (target_node)
				binder_inc_node(target_node, 1, 0, NULL);
			// 计算出存放flat_binder_object结构体偏移数组的起始地址，4字节对齐。
			offp = (size_t *)(t->buffer->data + ALIGN(tr->data_size, sizeof(void *)));
			   // struct flat_binder_object是binder在进程之间传输的表示方式 //
		       // 这里就是完成binder通讯单边时候在用户进程同内核buffer之间的一次拷贝动作 //
			  // 这里的数据拷贝，其实是拷贝到目标进程中去，因为t本身就是在目标进程的内核空间中分配的，
			if (copy_from_user(t->buffer->data, tr->data.ptr.buffer, tr->data_size)) {
				binder_user_error("binder: %d:%d got transaction with invalid "
					"data ptr\n", proc->pid, thread->pid);
				return_error = BR_FAILED_REPLY;
				goto err_copy_data_failed;
			}
			
可以看到binder_alloc_buf(target_proc, tr->data_size,tr->offsets_size, !reply && (t->flags & TF_ONE_WAY))函数在申请内存的时候，是从target_proc进程空间中去申请的，这样在做数据拷贝的时候copy_from_user(t->buffer->data, tr->data.ptr.buffer, tr->data_size))，就会直接拷贝target_proc的内核空间，而由于Binder内核空间的数据能直接映射到用户空间，这里就不在需要拷贝到用户空间。这就是一次拷贝的原理。内核空间的数据映射到用户空间其实就是添加一个偏移地址，并且将数据的首地址、数据的大小都复制到一个用户空间的Parcel结构体，具体可以参考Parcel.cpp的Parcel::ipcSetDataReference函数。

![Binder一次拷贝原理.jpg](http://upload-images.jianshu.io/upload_images/1460468-1f61b4f411c35094.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
			
## Binder传输数据的大小限制

虽然APP开发时候，Binder对程序员几乎不可见，但是作为Android的数据运输系统，Binder的影响是全面性的，所以有时候如果不了解Binder的一些限制，在出现问题的时候往往是没有任何头绪，比如在Activity之间传输BitMap的时候，如果Bitmap过大，就会引起问题，比如崩溃等，这其实就跟Binder传输数据大小的限制有关系，在上面的一次拷贝中分析过，mmap函数会为Binder数据传递映射一块连续的虚拟地址，这块虚拟内存空间其实是有大小限制的，不同的进程可能还不一样。

普通的由Zygote孵化而来的用户进程，所映射的Binder内存大小是不到1M的，准确说是 1*1024*1024) - (4096 *2) ：这个限制定义在ProcessState类中，如果传输说句超过这个大小，系统就会报错，因为Binder本身就是为了进程间频繁而灵活的通信所设计的，并不是为了拷贝大数据而使用的：

	#define BINDER_VM_SIZE ((1*1024*1024) - (4096 *2))

而在内核中，其实也有个限制，是4M，不过由于APP中已经限制了不到1M，这里的限制似乎也没多大用途：

	static int binder_mmap(struct file *filp, struct vm_area_struct *vma)
	{
		int ret;
		struct vm_struct *area;
		struct binder_proc *proc = filp->private_data;
		const char *failure_string;
		struct binder_buffer *buffer;
	    //限制不能超过4M
		if ((vma->vm_end - vma->vm_start) > SZ_4M)
			vma->vm_end = vma->vm_start + SZ_4M;
		。。。
		}
		
有个特殊的进程ServiceManager进程，它为自己申请的Binder内核空间是128K，这个同ServiceManager的用途是分不开的，ServcieManager主要面向系统Service，只是简单的提供一些addServcie，getService的功能，不涉及多大的数据传输，因此不需要申请多大的内存：

	int main(int argc, char **argv)
	{
	    struct binder_state *bs;
	    void *svcmgr = BINDER_SERVICE_MANAGER;
	
			// 仅仅申请了128k
	    bs = binder_open(128*1024);
	 if (binder_become_context_manager(bs)) {
	        ALOGE("cannot become context manager (%s)\n", strerror(errno));
	        return -1;
	    }
	
	    svcmgr_handle = svcmgr;
	    binder_loop(bs, svcmgr_handler);
	    return 0;
	}	
	
## 系统服务与bindService等启动的服务的区别

服务可分为系统服务与普通服务，系统服务一般是在系统启动的时候，由SystemServer进程创建并注册到ServiceManager中的。而普通服务一般是通过ActivityManagerService启动的服务，或者说通过四大组件中的Service组件启动的服务。这两种服务在实现跟使用上是有不同的，主要从以下几个方面：

* 服务的启动方式
* 服务的注册与管理
* 服务的请求使用方式

首先看一下服务的启动上，系统服务一般都是SystemServer进程负责启动，比如AMS，WMS，PKMS，电源管理等，这些服务本身其实实现了Binder接口，作为Binder实体注册到ServiceManager中，被ServiceManager管理，而SystemServer进程里面会启动一些Binder线程，主要用于监听Client的请求，并分发给响应的服务实体类，可以看出，这些系统服务是位于SystemServer进程中（有例外，比如Media服务）。在来看一下bindService类型的服务，这类服务一般是通过Activity的startService或者其他context的startService启动的，这里的Service组件只是个封装，主要的是里面Binder服务实体类，这个启动过程不是ServcieManager管理的，而是通过ActivityManagerService进行管理的，同Activity管理类似。

再来看一下服务的注册与管理：系统服务一般都是通过ServiceManager的addService进行注册的，这些服务一般都是需要拥有特定的权限才能注册到ServiceManager，而bindService启动的服务可以算是注册到ActivityManagerService，只不过ActivityManagerService管理服务的方式同ServiceManager不一样，而是采用了Activity的管理模型，详细的可以自行分析

最后看一下使用方式，使用系统服务一般都是通过ServiceManager的getService得到服务的句柄，这个过程其实就是去ServiceManager中查询注册系统服务。而bindService启动的服务，主要是去ActivityManagerService中去查找相应的Service组件，最终会将Service内部Binder的句柄传给Client。

![系统服务与bindService启动服务的区别.jpg](http://upload-images.jianshu.io/upload_images/1460468-122321a020b0ecb4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)	
	
##  Binder线程、Binder主线程、Client请求线程的概念与区别

Binder线程是执行Binder服务的载体，只对于服务端才有意义，对请求端来说，是不需要考虑Binder线程的，但Android系统的处理机制其实大部分是互为C/S的。比如APP与AMS进行交互的时候，都互为对方的C与S，这里先不讨论这个问题，先看Binder线程的概念。

Binder线程就是执行Binder实体业务的线程，一个普通线程如何才能成为Binder线程呢？很简单，只要开启一个监听Binder字符设备的Loop线程即可，在Android中有很多种方法，不过归根到底都是监听Binder，换成代码就是通过ioctl来进行监听。

拿ServerManager进程来说，其主线就是Binder线程，其做法是通过binder_loop实现不死线程：

	void binder_loop(struct binder_state *bs, binder_handler func)
	{
	   ...
	    for (;;) {
	    <!--关键点1-->
	        res = ioctl(bs->fd, BINDER_WRITE_READ, &bwr);
	     <!--关键点2-->
	        res = binder_parse(bs, 0, readbuf, bwr.read_consumed, func);
	        。。
	    }
	}
	
上面的关键代码1就是阻塞监听客户端请求，2 就是处理请求，并且这是一个死循环，不退出。再来看SystemServer进程中的线程，在Android4.3（6.0以后打代码就不一样了）中SystemSever主线程便是Binder线程，不过不是Binder主线程，Binder线程与Binder主线程的区别是：线程是否可以终止Loop，不过目前启动的Binder线程都是无法退出的，其实可以全部看做是Binder主线程，其实现原理是，**在SystemServer主线程执行到最后的时候，把自己编程Loop监听的Binder线程**，关键代码如下：

	extern "C" status_t system_init()
	{
	    ...
	    ALOGI("System server: entering thread pool.\n");
	    ProcessState::self()->startThreadPool();
	    IPCThreadState::self()->joinThreadPool();
	    ALOGI("System server: exiting thread pool.\n");
	    return NO_ERROR;
	}

ProcessState::self()->startThreadPool()是新建一个Binder主线程，而PCThreadState::self()->joinThreadPool()是将当前线程变成Binder线程。其实startThreadPool最终也会调用joinThreadPool，看下其关键函数：

	void IPCThreadState::joinThreadPool(bool isMain)
	{
	 	...
	    status_t result;
	    do {
	        int32_t cmd;
	      	...关键点1 
	        result = talkWithDriver();
	        if (result >= NO_ERROR) {
	           ...关键点2 
	            result = executeCommand(cmd);
	        }
	        // 非主线程的可以退出
	        if(result == TIMED_OUT && !isMain) {
	            break;
	        }
	        // 死循环，不完结，调用了这个，就好比是开启了Binder监听循环，
	    } while (result != -ECONNREFUSED && result != -EBADF);
	 }
 
	status_t IPCThreadState::talkWithDriver(bool doReceive)
	{  
	    do {
	        ...关键点3 
	        if (ioctl(mProcess->mDriverFD, BINDER_WRITE_READ, &bwr) >= 0)
	   }   
	     
先看关键点1 talkWithDriver，其实质还是去掉用ioctl(mProcess->mDriverFD, BINDER_WRITE_READ, &bwr) >= 0)去不断的监听Binder字符设备，获取到Client传输的数据后，再通过executeCommand去执行相应的请求，joinThreadPool是普通线程化身Binder线程最常见的方式。不信，就再看一个MediaService,看一下main_mediaserver的main函数：

	
	int main(int argc, char** argv)
	{
	   。。。
	        sp<ProcessState> proc(ProcessState::self());
	        sp<IServiceManager> sm = defaultServiceManager();
	        ALOGI("ServiceManager: %p", sm.get());
	        AudioFlinger::instantiate();
	        MediaPlayerService::instantiate();
	        CameraService::instantiate();
	        AudioPolicyService::instantiate();
	        registerExtensions();
	        ProcessState::self()->startThreadPool();
	        IPCThreadState::self()->joinThreadPool();
	    }
    
其实还是通过joinThreadPool变身Binder线程，至于是不是主线程，看一下下面的函数：
	
	void IPCThreadState::joinThreadPool(bool isMain)

	void ProcessState::spawnPooledThread(bool isMain)
	{
	    if (mThreadPoolStarted) {
	        String8 name = makeBinderThreadName();
	        ALOGV("Spawning new pooled thread, name=%s\n", name.string());
	        sp<Thread> t = new PoolThread(isMain);
	        t->run(name.string());
	    }
	}

其实就是传递给joinThreadPool函数的isMain是否是true，不过是否是Binder主线程并没有什么用，因为源码中并没有为这两者的不同处理留入口，感兴趣可以去查看一下binder中的TIMED_OUT。

最后来看一下普通Client的binder请求线程，比如我们APP的主线程，在startActivity请求AMS的时候，APP的主线程成其实就是Binder请求线程，在进行Binder通信的过程中，Client的Binder请求线程会一直阻塞，知道Service处理完毕返回处理结果。
 
##  Binder请求的同步与异步

很多人都会说，Binder是对Client端同步，而对Service端异步，其实并不完全正确，在单次Binder数据传递的过程中，其实都是同步的。只不过，Client在请求Server端服务的过程中，是需要返回结果的，即使是你看不到返回数据，其实还是会有个成功与失败的处理结果返回给Client，这就是所说的Client端是同步的。至于说服务端是异步的，可以这么理解：在服务端在被唤醒后，就去处理请求，处理结束后，服务端就将结果返回给正在等待的Client线程，将结果写入到Client的内核空间后，服务端就会直接返回了，不会再等待Client端的确认，这就是所说的服务端是异步的，可以从源码来看一下：

* Client端同步阻塞请求 

	status_t IPCThreadState::transact(int32_t handle,
	                                  uint32_t code, const Parcel& data,
	                                  Parcel* reply, uint32_t flags) 
	 {
	   
	        if (reply) {
	            err = waitForResponse(reply);
	        } ...
 
Client在请求服务的时候 Parcel* reply基本都是非空的（还没见过空用在什么位置），非空就会执行waitForResponse(reply)，如果看过几篇Binder分析文章的人应该都会知道，在A端向B写完数据之后，A会返回给自己一个BR_TRANSACTION_COMPLETE命令，告知自己数据已经成功写入到B的Binder内核空间中去了，如果是需要回复，在处理完BR_TRANSACTION_COMPLETE命令后会继续阻塞等待结果的返回：

	status_t IPCThreadState::waitForResponse(Parcel *reply, status_t *acquireResult){
	    ...
	    while (1) {
		if ((err=talkWithDriver()) < NO_ERROR) break;
		 cmd = mIn.readInt32();
	    switch (cmd) {
		   <!--关键点1 -->
		  case BR_TRANSACTION_COMPLETE:
	            if (!reply && !acquireResult) goto finish;
	            break;
	     <!--关键点2 -->
	        case BR_REPLY:
	            {
	                binder_transaction_data tr;
	                  // free buffer，先设置数据，直接
	                if (reply) {
	                    if ((tr.flags & TF_STATUS_CODE) == 0) {
	                        // 牵扯到数据利用，与内存释放
	                        reply->ipcSetDataReference(...)
	            }
	            goto finish;
	    }
	 finish:
     ...
    return err;
	}

关键点1就是处理BR_TRANSACTION_COMPLETE，如果需要等待reply，还要通过talkWithDriver等待结果返回，最后执行关键点2，处理返回数据。**对于服务端来说，区别就在于关键点1 **，来看一下服务端Binder线程的代码，拿常用的joinThreadPool来看，在talkWithDriver后，会执行executeCommand函数，

	void IPCThreadState::joinThreadPool(bool isMain)
	{
	 	...
	    status_t result;
	    do {
	        int32_t cmd;
	      	...关键点1 
	        result = talkWithDriver();
	        if (result >= NO_ERROR) {
	           ...关键点2 
	            result = executeCommand(cmd);
	        }
	        // 非主线程的可以退出
	        if(result == TIMED_OUT && !isMain) {
	            break;
	        }
	        // 死循环，不完结，调用了这个，就好比是开启了Binder监听循环，
	    } while (result != -ECONNREFUSED && result != -EBADF);
	 }
	 
executeCommand会进一步调用sendReply函数，看一下这里的特点waitForResponse(NULL, NULL)，这里传递的都是null，在上面的关键点1的地方我们知道，这里不需要等待Client返回，因此会直接 goto finish，这就是所说的Client同步，而服务端异步的逻辑。

	// BC_REPLY
	status_t IPCThreadState::sendReply(const Parcel& reply, uint32_t flags)
	{
	    // flag 0
	    status_t err;
	    status_t statusBuffer;
	    err = writeTransactionData(BC_REPLY, flags, -1, 0, reply, &statusBuffer);
	    if (err < NO_ERROR) return err;
	    return waitForResponse(NULL, NULL);
	}

<!--粘贴一下上面的代码-->
	  case BR_TRANSACTION_COMPLETE:
            if (!reply && !acquireResult) goto finish;
            break;
	            
请求同步最好的例子就是在Android6.0之前，国产ROM权限的申请都是同步的，在申请权限的时候，APP申请权限的线程会阻塞，就算是UI线程也会阻塞，ROM为了防止ANR，都会为权限申请设置一个倒计时，不操作，就给个默认操作，有兴趣可以自己分析。

## Android APP进程天生支持Binder通信的原理是什么

Android APP进程都是由Zygote进程孵化出来的。常见场景：点击桌面icon启动APP，或者startActivity启动一个新进程里面的Activity，最终都会由AMS去调用Process.start()方法去向Zygote进程发送请求，让Zygote去fork一个新进程，Zygote收到请求后会调用Zygote.forkAndSpecialize()来fork出新进程,之后会通过RuntimeInit.nativeZygoteInit来初始化Andriod APP运行需要的一些环境，而binder线程就是在这个时候新建启动的，看下面的源码（Android 4.3）：

这里不分析Zygote，只是给出其大概运行机制，Zygote在启动后，就会通过runSelectLoop不断的监听socket，等待请求来fork进程，如下：

    private static void runSelectLoop() throws MethodAndArgsCaller {
        ArrayList<FileDescriptor> fds = new ArrayList<FileDescriptor>();
        ArrayList<ZygoteConnection> peers = new ArrayList<ZygoteConnection>();
        FileDescriptor[] fdArray = new FileDescriptor[4];
		...     
       int loopCount = GC_LOOP_COUNT;
        while (true) {
            int index;
             ...
                boolean done;
                done = peers.get(index).runOnce();
				...
            }}}
每次fork请求到来都会调用ZygoteConnection的runOnce()来处理请求，
  
    boolean runOnce() throws ZygoteInit.MethodAndArgsCaller {

        String args[];
        Arguments parsedArgs = null;
        FileDescriptor[] descriptors;	
			。。。

        try {
           ...关键点1 
            pid = Zygote.forkAndSpecialize(parsedArgs.uid, parsedArgs.gid, parsedArgs.gids,
                    parsedArgs.debugFlags, rlimits, parsedArgs.mountExternal, parsedArgs.seInfo,
                    parsedArgs.niceName);
        } 
        try {
            if (pid == 0) {
                // in child
             ...关键点2
                handleChildProc(parsedArgs, descriptors, childPipeFd, newStderr);
			。。。
        }
         
runOnce()有两个关键点，**关键点1** Zygote.forkAndSpecialize就是通过fork系统调用来新建进程，**关键点2 **handleChildProc就是对新建的APP进程进行一些初始化工作，为Android Java进程创建一些必须的场景。Zygote.forkAndSpecialize没什么可看的，就是Linux中的fork进程，这里主要看一下handleChildProc

    private void handleChildProc(Arguments parsedArgs,
            FileDescriptor[] descriptors, FileDescriptor pipeFd, PrintStream newStderr)
            throws ZygoteInit.MethodAndArgsCaller {
            
		//从Process.start启动的parsedArgs.runtimeInit一般都是true    		 if (parsedArgs.runtimeInit) {
            if (parsedArgs.invokeWith != null) {
                WrapperInit.execApplication(parsedArgs.invokeWith,
                        parsedArgs.niceName, parsedArgs.targetSdkVersion,
                        pipeFd, parsedArgs.remainingArgs);  
            } else {
           	 // Android应用启动都走该分支
           RuntimeInit.zygoteInit(parsedArgs.targetSdkVersion,
                        parsedArgs.remainingArgs); 
           }  
    }

接着看 RuntimeInit.zygoteInit函数

	public static final void zygoteInit(int targetSdkVersion, String[] argv)
	        throws ZygoteInit.MethodAndArgsCaller {

	    redirectLogStreams();
	    commonInit();
	    <!--关键点1-->
	    nativeZygoteInit();
	     <!--关键点2-->
	    applicationInit(targetSdkVersion, argv);
	}


先看关键点1，nativeZygoteInit属于Native方法，该方法位于AndroidRuntime.cpp中，其实就是调用调用到app_main.cpp中的onZygoteInit

	static void com_android_internal_os_RuntimeInit_nativeZygoteInit(JNIEnv* env, jobject clazz)
	{
	    gCurRuntime->onZygoteInit();
	}

**关键就是onZygoteInit**
	
		virtual void onZygoteInit()
		{
		    sp proc = ProcessState::self();
		    //启动新binder线程loop
		    proc->startThreadPool();
		}
		
首先，ProcessState::self()函数会调用open()打开/dev/binder设备，这个时候Client就能通过Binder进行远程通信；其次，proc->startThreadPool()负责新建一个binder线程，监听Binder设备，这样进程就具备了作为Binder服务端的资格。每个APP的进程都会通过onZygoteInit打开Binder，既能作为Client，也能作为Server，这就是Android进程天然支持Binder通信的原因。

![Android APP进程天然支持Binder通信.png](http://upload-images.jianshu.io/upload_images/1460468-8236f100bc8a5bd1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## Android APP有多少Binder线程，是固定的么？

通过上一个问题我们知道了Android APP线程为什么天然支持Binder通信，并且可以作为Binder的Service端，同时也对Binder线程有了一个了解，那么在一个Android APP的进程里面究竟有多少个Binder线程呢？是固定的吗。在分析上一个问题的时候，我们知道Android APP进程在Zygote fork之初就为它新建了一个Binder主线程，使得APP端也可以作为Binder的服务端，这个时候Binder线程的数量就只有一个，假设我们的APP自身实现了很多的Binder服务，一个线程够用的吗？这里不妨想想一下SystemServer进程，SystemServer拥有很多系统服务，一个线程应该是不够用的，如果看过SystemServer代码可能会发现，对于Android4.3的源码，其实一开始为该服务开启了两个Binder线程。还有个分析Binder常用的服务，media服务，也是在一开始的时候开启了两个线程。

先看下SystemServer的开始加载的线程：通过 ProcessState::self()->startThreadPool()新加了一个Binder线程，然后通过IPCThreadState::self()->joinThreadPool();将当前线程变成Binder线程，注意这里是针对Android4.3的源码，android6.0的这里略有不同。

	extern "C" status_t system_init()
	{
	    ...
	    ALOGI("System server: entering thread pool.\n");
	    ProcessState::self()->startThreadPool();
	    IPCThreadState::self()->joinThreadPool();
	    ALOGI("System server: exiting thread pool.\n");
	    return NO_ERROR;
	}

再看下Media服务,同SystemServer类似，也是开启了两个Binder线程：

	int main(int argc, char** argv)
	{      ...
	        ProcessState::self()->startThreadPool();
	        IPCThreadState::self()->joinThreadPool();
	 }	

可以看出Android APP上层应用的进程一般是开启一个Binder线程，而对于SystemServer或者media服务等使用频率高，服务复杂的进程，一般都是开启两个或者更多。来看第二个问题，**Binder线程的数目是固定的吗？答案是否定的，**驱动会根据目标进程中是否存在足够多的Binder线程来告诉进程是不是要新建Binder线程，详细逻辑，首先看一下新建Binder线程的入口：

	status_t IPCThreadState::executeCommand(int32_t cmd)
	{
	    BBinder* obj;
	    RefBase::weakref_type* refs;
	    status_t result = NO_ERROR;
	    switch (cmd) {
	    ...
	    // 可以根据内核返回数据创建新的binder线程
	    case BR_SPAWN_LOOPER:
	        mProcess->spawnPooledThread(false);
	        break;
    }
    
executeCommand一定是从Bindr驱动返回的BR命令，这里是BR_SPAWN_LOOPER，什么时候，Binder驱动会向进程发送BR_SPAWN_LOOPER呢？全局搜索之后，发现只有一个地方binder_thread_read，


	binder_thread_read（）{
	  ...
	retry:
	    //当前线程todo队列为空且transaction栈为空，则代表该线程是空闲的
	    wait_for_proc_work = thread->transaction_stack == NULL &&
	        list_empty(&thread->todo);
	  
	    if (thread->return_error != BR_OK && ptr < end) {
	        ...
	        put_user(thread->return_error, (uint32_t __user *)ptr);
	        ptr += sizeof(uint32_t);
	        goto done; //发生error，则直接进入done
	    }
	    
	    thread->looper |= BINDER_LOOPER_STATE_WAITING;
	    if (wait_for_proc_work)
	        proc->ready_threads++; //可用线程个数+1
	    binder_unlock(__func__);
	    
	    if (wait_for_proc_work) {
	        if (non_block) {
	            ...
	        } else
	            //当进程todo队列没有数据,则进入休眠等待状态
	            ret = wait_event_freezable_exclusive(proc->wait, binder_has_proc_work(proc, thread));
	    } else {
	        if (non_block) {
	            ...
	        } else
	            //当线程todo队列没有数据，则进入休眠等待状态
	            ret = wait_event_freezable(thread->wait, binder_has_thread_work(thread));
	    }
	    
	    binder_lock(__func__);
	    if (wait_for_proc_work)
	        proc->ready_threads--; //可用线程个数-1
	    thread->looper &= ~BINDER_LOOPER_STATE_WAITING;
	    
	    if (ret)
	        return ret; //对于非阻塞的调用，直接返回
	  
	    while (1) {
	        uint32_t cmd;
	        struct binder_transaction_data tr;
	        struct binder_work *w;
	        struct binder_transaction *t = NULL;
	        
	        //先考虑从线程todo队列获取事务数据
	        if (!list_empty(&thread->todo)) {
	            w = list_first_entry(&thread->todo, struct binder_work, entry);
	        //线程todo队列没有数据, 则从进程todo对获取事务数据
	        } else if (!list_empty(&proc->todo) && wait_for_proc_work) {
	            w = list_first_entry(&proc->todo, struct binder_work, entry);
	        } else {
	            ... //没有数据,则返回retry
	        }
	
	        switch (w->type) {
	            case BINDER_WORK_TRANSACTION: ...  break;
	            case BINDER_WORK_TRANSACTION_COMPLETE:...  break;
	            case BINDER_WORK_NODE: ...    break;
	            case BINDER_WORK_DEAD_BINDER:
	            case BINDER_WORK_DEAD_BINDER_AND_CLEAR:
	            case BINDER_WORK_CLEAR_DEATH_NOTIFICATION:
	                struct binder_ref_death *death;
	                uint32_t cmd;
	
	                death = container_of(w, struct binder_ref_death, work);
	                if (w->type == BINDER_WORK_CLEAR_DEATH_NOTIFICATION)
	                  cmd = BR_CLEAR_DEATH_NOTIFICATION_DONE;
	                else
	                  cmd = BR_DEAD_BINDER;
	                put_user(cmd, (uint32_t __user *)ptr;
	                ptr += sizeof(uint32_t);
	                put_user(death->cookie, (void * __user *)ptr);
	                ptr += sizeof(void *);
	                ...
	                if (cmd == BR_DEAD_BINDER)
	                  goto done; //Binder驱动向client端发送死亡通知，则进入done
	                break;
	        }
	
	        if (!t)
	            continue; //只有BINDER_WORK_TRANSACTION命令才能继续往下执行
	
	        if (t->buffer->target_node) {
	            ...
	            cmd = BR_TRANSACTION;  //设置命令为BR_TRANSACTION
	        } else {
	            ...
	            cmd = BR_REPLY; //设置命令为BR_REPLY
	        }
	        ...
	
	        //将cmd和数据写回用户空间
	        if (put_user(cmd, (uint32_t __user *)ptr))  return -EFAULT;
	        ptr += sizeof(uint32_t);
	        if (copy_to_user(ptr, &tr, sizeof(tr)))   return -EFAULT;
	        ptr += sizeof(tr);
	        ...
	        break;
	    }
	    
	done:
	    *consumed = ptr - buffer;
	    //创建线程的条件
	    if (proc->requested_threads + proc->ready_threads == 0 &&
	        proc->requested_threads_started < proc->max_threads &&
	        (thread->looper & (BINDER_LOOPER_STATE_REGISTERED |
	         BINDER_LOOPER_STATE_ENTERED))) {
	        proc->requested_threads++;
	        // 生成BR_SPAWN_LOOPER命令，用于创建新的线程
	        put_user(BR_SPAWN_LOOPER, (uint32_t __user *)buffer)；
	    }
	    return 0;
	}

 当发生以下3种情况之一，便会进入done：

当前线程的return_error发生error的情况；
当Binder驱动向client端发送死亡通知的情况；
当类型为BINDER_WORK_TRANSACTION(即收到命令是BC_TRANSACTION或BC_REPLY)的情况；
任何一个Binder线程当同时满足以下条件，则会生成用于创建新线程的BR_SPAWN_LOOPER命令：

当前进程中没有请求创建binder线程，即requested_threads = 0；
当前进程没有空闲可用的binder线程，即ready_threads = 0；（线程进入休眠状态的个数就是空闲线程数）
当前进程已启动线程个数小于最大上限(默认15)；
当前线程已接收到BC_ENTER_LOOPER或者BC_REGISTER_LOOPER命令，即当前处于BINDER_LOOPER_STATE_REGISTERED或者BINDER_LOOPER_STATE_ENTERED状态。【小节2.6】已设置状态为BINDER_LOOPER_STATE_ENTERED，显然这条件是满足的。
从system_server的binder线程一直的执行流: IPC.joinThreadPool –> IPC.getAndExecuteCommand() -> IPC.talkWithDriver() ,但talkWithDriver收到事务之后, 便进入IPC.executeCommand(), 接下来,从executeCommand说起.


# Binder的执行线程与注册线程 

# Binder的查询


看看是不是返回，看看是不是有相互等待，因为Transaction栈是先处理上面的，如果存在相互等待，那么一定不会冲突

# 发送请求的的时候，阻塞在那个队列上，是自己线程的队列，还是进程，依据是什么


依据是，当前没有发送请求，并且是没有待处理任务，一般来说，这种情况是对Server端的，

猜测：比如多个请求，发送到进程队列上去了，这样多个线程能同时响应，也能处理快一些，

	// thread->transaction_stack  是否有请求压栈
	// 是否是当前线程没有需要处理的事情
	wait_for_proc_work = thread->transaction_stack == NULL &&
				list_empty(&thread->todo);
	if (thread->return_error != BR_OK && ptr < end) {
	 // $￥
	}
	// 表明线程即将进入等待状态。
	thread->looper |= BINDER_LOOPER_STATE_WAITING;
	// 就绪等待任务的空闲线程数加1。
	if (wait_for_proc_work)
		proc->ready_threads++;
	binder_unlock(__func__);
	trace_binder_wait_for_work(wait_for_proc_work,
				   !!thread->transaction_stack,
				   !list_empty(&thread->todo));
	if (wait_for_proc_work) {
		// 进程等待, 
		if (!(thread->looper & (BINDER_LOOPER_STATE_REGISTERED |
					BINDER_LOOPER_STATE_ENTERED))) {
			binder_user_error("binder: %d:%d ERROR: Thread waiting "
				"for process work before calling BC_REGISTER_"
				"LOOPER or BC_ENTER_LOOPER (state %x)\n",
				proc->pid, thread->pid, thread->looper);
			wait_event_interruptible(binder_user_error_wait,
						 binder_stop_on_user_error < 2);
		}
		binder_set_nice(proc->default_priority);
		if (non_block) {
			if (!binder_has_proc_work(proc, thread))
				// 返回try again的提示。
				ret = -EAGAIN;
		} else
			// 当前task互斥等待在进程全局的等待队列中。 
			// 当前task互斥等待在进程全局的等待队列中。 
			// 多个线程互斥等待，防止重复处理请求
			 // 当前task互斥等待在进程全局的等待队列中。 
			// 多个线程互斥等待，防止重复处理请求
			// 何谓排他性的等待？有一些进程都在等待队列中，当唤醒的时候，
			// 内核是唤醒所有的进程。如果进程设置了排他性等待的标志，
			// 唤醒所有非排他性的进程和一个排他性进程。线程的排他性，其实都是线程，线程是内核调度的最小单位
			ret = wait_event_freezable_exclusive(proc->wait, binder_has_proc_work(proc, thread));
	} else {
		// 线程等待
		if (non_block) {
			if (!binder_has_thread_work(thread))
				ret = -EAGAIN;
		} else
		 /* 当前task等待在task自己的等待队列中(binder_thread.todo)，永远只有其自己。，只有自己*/
		 ret = wait_event_freezable(thread->wait, binder_has_thread_work(thread));
	}
	
**看看是不是有请求，或者有待处理的事情	再决定插入到哪里**


# Binder驱动中的四棵树

# 如何定向导弹

标记线程挂起，用哪个线程通信，那个才是Binder线程，并非所有的线程都是，只有open了驱动的进程，并且阻塞读取的线程，才算是Binder线程。

# APP层面有几个线程

至少Binder线程是Zygote分离时候就自带的

红黑树节点的产生过程

另一个要考虑的东西就是binder_proc里的那4棵树啦。前文在阐述binder_get_thread()时，已经看到过向threads树中添加节点的动作。那么其他3棵树的节点该如何添加呢？其实，秘密都在传输动作中。要知道，binder驱动在传输数据的时候，可不是仅仅简单地递送数据噢，它会分析被传输的数据，找出其中记录的binder对象，并生成相应的树节点。如果传输的是个binder实体对象，它不仅会在发起端对应的nodes树中添加一个binder_node节点，还会在目标端对应的refs_by_desc树、refs_by_node树中添加一个binder_ref节点，而且让binder_ref节点的node域指向binder_node节点

[](http://static.oschina.net/uploads/img/201308/15213415_Dm2n.png)

![](http://static.oschina.net/uploads/img/201308/15213415_Dm2n.png)

使用及添加时机不同，refs_by_desc主要是用在客户端使用的时候，refs_by_node主要是在getService添加的时候

基本上都会使用进程的队列，为什呢？，因为写数据之后，先返回一些响应，之后再用空的进行写，这个时候会阻塞在自己的进程队列中



	
	// 根据32位的uint32_t desc来查找
	
	static struct binder_ref *binder_get_ref(struct binder_proc *proc,
						 uint32_t desc)
	{
		struct rb_node *n = proc->refs_by_desc.rb_node;
		struct binder_ref *ref;
		while (n) {
			ref = rb_entry(n, struct binder_ref, rb_node_desc);
			if (desc < ref->desc)
				n = n->rb_left;
			else if (desc > ref->desc)
				n = n->rb_right;
			else
				return ref;
		}
		return NULL;
	}
	
	
	// 为何
	static struct binder_ref *binder_get_ref_for_node(struct binder_proc *proc,
							  struct binder_node *node)
	{
		struct rb_node *n;
		struct rb_node **p = &proc->refs_by_node.rb_node;
		struct rb_node *parent = NULL;
		struct binder_ref *ref, *new_ref;
		while (*p) {
			parent = *p;
			ref = rb_entry(parent, struct binder_ref, rb_node_node);
			if (node < ref->node)
				p = &(*p)->rb_left;
			else if (node > ref->node)
				p = &(*p)->rb_right;
			else
				return ref;
		}
	
		// binder_ref 可以在两棵树里面，但是，两棵树的查询方式不同，并且通过desc查询，不具备新建功能
	
		new_ref = kzalloc(sizeof(*ref), GFP_KERNEL);
		if (new_ref == NULL)
			return NULL;
		binder_stats_created(BINDER_STAT_REF);
		new_ref->debug_id = ++binder_last_id;
		new_ref->proc = proc;
		new_ref->node = node;
		rb_link_node(&new_ref->rb_node_node, parent, p);
		
		// 插入到proc->refs_by_node红黑树中去
	
		rb_insert_color(&new_ref->rb_node_node, &proc->refs_by_node);
		
		// 是不是ServiceManager的
		new_ref->desc = (node == binder_context_mgr_node) ? 0 : 1;
	
		// 分配Handle句柄，为了插入到refs_by_desc
		for (n = rb_first(&proc->refs_by_desc); n != NULL; n = rb_next(n)) {
			ref = rb_entry(n, struct binder_ref, rb_node_desc);
			if (ref->desc > new_ref->desc)
				break;
			new_ref->desc = ref->desc + 1;
		}
		// 插入到refs_by_desc红黑树中区
		p = &proc->refs_by_desc.rb_node;
		while (*p) {
			parent = *p;
			ref = rb_entry(parent, struct binder_ref, rb_node_desc);
			if (new_ref->desc < ref->desc)
				p = &(*p)->rb_left;
			else if (new_ref->desc > ref->desc)
				p = &(*p)->rb_right;
			else
				BUG();
		}
		rb_link_node(&new_ref->rb_node_desc, parent, p);
			// 插入到refs_by_desc红黑树中区
		rb_insert_color(&new_ref->rb_node_desc, &proc->refs_by_desc);
	
	
		if (node) {
			hlist_add_head(&new_ref->node_entry, &node->refs);
			binder_debug(BINDER_DEBUG_INTERNAL_REFS,
				     "binder: %d new ref %d desc %d for "
				     "node %d\n", proc->pid, new_ref->debug_id,
				     new_ref->desc, node->debug_id);
		} else {
			binder_debug(BINDER_DEBUG_INTERNAL_REFS,
				     "binder: %d new ref %d desc %d for "
				     "dead node\n", proc->pid, new_ref->debug_id,
				      new_ref->desc);
		}
		return new_ref;
	}



很多分析将Binder框架定义了四个角色：Server，Client，ServiceManager（以后简称SMgr）以及Binder驱动，其实这是容易将人引导到歧途，比如我们平时使用AIDL定义服务，并通信的时候，也许你觉得是注册到ServiceManager，其实不是，用户的Service是ActivityManagerService负责的。

其中Server，Client，SMgr运行于用户空间，驱动运行于内核空间。这四个角色的关系和互联网类似：Server是服务器，Client是客户终端，SMgr是域名服务器（DNS），驱动是路由器。


Bp flags = 0 单向

  BpBinder.h
      virtual status_t    transact(   uint32_t code,
                                    const Parcel& data,
                                    Parcel* reply,
                                    uint32_t flags = 0);
                                    
                                    
     // TF_ONE_WAY == 1 
    // 并非单向，阻塞请求
    if ((flags & TF_ONE_WAY) == 0) {
        #if 0
        if (code == 4) { // relayout
            ALOGI(">>>>>> CALLING transaction 4");
        } else {
            ALOGI(">>>>>> CALLING transaction %d", code);
        }
        #endif
        if (reply) {
            err = waitForResponse(reply);
        } else {
            Parcel fakeReply;
            err = waitForResponse(&fakeReply);
        }
        #if 0
        if (code == 4) { // relayout
            ALOGI("<<<<<< RETURNING transaction 4");
        } else {
            ALOGI("<<<<<< RETURNING transaction %d", code);
        }
        #endif
        
        IF_LOG_TRANSACTIONS() {
            TextOutput::Bundle _b(alog);
            alog << "BR_REPLY thr " << (void*)pthread_self() << " / hand "
                << handle << ": ";
            if (reply) alog << indent << *reply << dedent << endl;
            else alog << "(none requested)" << endl;
        }
    } else {
        err = waitForResponse(NULL, NULL);
    }
    
    return err;
    
    
    
    
    
        // Is the read buffer empty?
    const bool needRead = mIn.dataPosition() >= mIn.dataSize();
    
    // We don't want to write anything if we are still reading
    // from data left in the input buffer and the caller
    // has requested to read the next data.

 

    // 如果正在读取数据，就不要再写请求，因为可能同时返回多个返回，一次性处理多个，先通知发完了，并且没有返回，

    const size_t outAvail = (!doReceive || needRead) ? mOut.dataSize() : 0;
    
    bwr.write_size = outAvail;
    bwr.write_buffer = (long unsigned int)mOut.data();

    // This is what we'll read.
    if (doReceive && needRead) {
        // 在这里把接受数据的size跟大小获取到，注意一次获取的大小，传递的大小，其实不用传递mIn，mOut，传递数据大小及地址就行
        bwr.read_size = mIn.dataCapacity();
        bwr.read_buffer = (long unsigned int)mIn.data();
    } else {
        bwr.read_size = 0;
        bwr.read_buffer = 0;
    }
    
    

(08) 在跳出while循环之后，会更新consumed的值。即，更新bwr.read_consumed的值。此时，由于写入了BR_NOOP和BR_TRANSACTION_COMPLETE两个指令，bwr.read_consumed=8。
    
说明： (01) 此时，因为在waitForResponse()中已经通过mIn.readInt32()读取了4个字节，因此mIn.dataPosition()=4，而mIn.dataSize()=8；因此，needRead=false。
(02) needRead=false，而doReceive=true；因此，outAvail=0。
最终，由于 bwr.write_size和bwr.read_size都为0，因此直接返回NO_ERROR。
再次回到waitForResponse()中，此时读出的cmd为BR_TRANSACTION_COMPLETE。此时，由于reply不为NULL，因此再次重新执行while循环，调用talkWithDriver()。
(01) 此时，已经读取了mIn中的全部数据，因此mIn.dataPosition()=8，而mIn.dataSize()=8；因此，needRead=true。
(02) outAvail=mOut.dataSize()，前面已经将mOut清空，因此outAvail=0。bwr初始化完毕之后，各个成员的值如下：


[参考文档](http://wangkuiwu.github.io/2014/09/05/BinderCommunication-AddService01/)
    
 
		
		
 
#  几个重要的结构体

binder_work等待处理的事件队列

 
binder_transaction    事件内容


为什么分开binder_transaction是复用的 binder_work是独立的，每个线程独有的这样处理比较好
    
    
#     binder主线程与其余Binder线程有什么不同

Binder系统中可分为3类binder线程：

Binder主线程：进程创建过程会调用startThreadPool()过程中再进入spawnPooledThread(true)，来创建Binder主线程。编号从1开始，也就是意味着binder主线程名为binder_1，并且主线程是不会退出的。
Binder普通线程：是由Binder Driver来根据是否有空闲的binder线程来决定是否创建binder线程，回调spawnPooledThread(false) ，isMain=false，该线程名格式为binder_x。
Binder其他线程：其他线程是指并没有调用spawnPooledThread方法，而是直接调用IPC.joinThreadPool()，将当前线程直接加入binder线程队列。例如： mediaserver和servicemanager的主线程都是binder线程，但system_server的主线程并非binder线程。


Binder的transaction有3种类型：

call: 发起进程的线程不一定是在Binder线程， 接收者只指向进程，并不确定会有哪个线程来处理，所以不指定线程；
reply: 发起者一定是binder线程，并且接收者线程便是上次call时的发起线程(该线程不一定是binder线程，可以是任意线程)。
async: 与call类型差不多，唯一不同的是async是oneway方式不需要回复，发起进程的线程不一定是在Binder线程， 接收者只指向进程，并不确定会有哪个线程来处理，所以不指定线程。


# Binder 线程的自动扩容 

	// Bn 会阻塞等待在这等待 Bp 的请求的到来
		static int binder_thread_read(struct binder_proc *proc,
		                  struct binder_thread *thread,
		                  void  __user *buffer, int size,
		                  signed long *consumed, int non_block)
		{
		    void __user *ptr = buffer + *consumed;
		    void __user *end = buffer + size;
		    int ret = 0;
		    int wait_for_proc_work;
		    if (*consumed == 0) {
		        if (put_user(BR_NOOP, (uint32_t __user *)ptr))
		            return -EFAULT;
		        ptr += sizeof(uint32_t);
		    }
		retry:
		    // transaction_stack == NULL 代表是第一次的 read（Bn 的阻塞read就是）
		    // Bn 的阻塞等待的 read todo list 也是空的
		    // 所以 Bn 的阻塞 read 这里的 wait_for_proc_work 是 true
		    wait_for_proc_work = thread->transaction_stack == NULL &&
		                list_empty(&thread->todo);
		    if (thread->return_error != BR_OK && ptr < end) {
		        if (thread->return_error2 != BR_OK) {
		            if (put_user(thread->return_error2, (uint32_t __user *)ptr))
		                return -EFAULT;
		            ptr += sizeof(uint32_t);
		            if (ptr == end)
		                goto done;
		            thread->return_error2 = BR_OK;
		        }
		        if (put_user(thread->return_error, (uint32_t __user *)ptr))
		            return -EFAULT;
		        ptr += sizeof(uint32_t);
		        thread->return_error = BR_OK;
		        goto done;
		    }
		    // 前面说了这个 looper 是当前线程的状态，
		    // 注意这里设置为 WAITING 了，表示正在等待
		    thread->looper |= BINDER_LOOPER_STATE_WAITING;
		    // Bn read 这里是 true，表示本进程空闲的进程数加1
		    if (wait_for_proc_work)
		        proc->ready_threads++;
		    mutex_unlock(&binder_lock);
		    if (wait_for_proc_work) {
		        // 这里检测 thread 是不是有下面这2个标志，这2个标志后面会说到。
		        // 还有注意前面设置那个 WAITTING 的是用 | 设置的，然后这里检测是用 &
		        // 然后看看这几个标志定义的值，会发现这里微妙的用法
		        if (!(thread->looper & (BINDER_LOOPER_STATE_REGISTERED |
		                    BINDER_LOOPER_STATE_ENTERED))) {
		            binder_user_error("binder: %d:%d ERROR: Thread waiting "
		                "for process work before calling BC_REGISTER_"
		                "LOOPER or BC_ENTER_LOOPER (state %x)\n",
		                proc->pid, thread->pid, thread->looper);
		            wait_event_interruptible(binder_user_error_wait,
		                         binder_stop_on_user_error < 2);
		        }
		        binder_set_nice(proc->default_priority);
		        if (non_block) {
		            if (!binder_has_proc_work(proc, thread))
		                ret = -EAGAIN;
		        } else
		            // 这里就阻塞在这里，等 thread 的 todo list 不为空（Bp 请求）
		            ret = wait_event_interruptible_exclusive(proc->wait, binder_has_proc_work(proc, thread));
		    } else {
		        if (non_block) {
		            if (!binder_has_thread_work(thread))
		                ret = -EAGAIN;
		        } else
		            ret = wait_event_interruptible(thread->wait, binder_has_thread_work(thread));
		    }
		    mutex_lock(&binder_lock);
		    // 如果这个等待的线程被唤醒了（有 Bp 请求来了），
		    // 把这个进程空闲的线程数减1，
		    // 因为这个线程后面马上就要到用户空间去执行相关业务的函数了。
		    if (wait_for_proc_work)
		        proc->ready_threads--;
		    // 把线程的 WAITTING 标志去掉
		    thread->looper &= ~BINDER_LOOPER_STATE_WAITING;
		    // wait 出错的话，返回错误值
		    if (ret)
		        return ret;
		... ...
		done:
		    *consumed = ptr - buffer;
		    // 最后这里 requested_threads 表示发出请求要启动的线程数，
		    // ready_threads 表示空闲的线程数。
		    // 如果这2个加起来 == 0 就表示当前进程（服务进程）没有空闲的线程来处理请求，
		    // 并且还没请求去启动线程，所以需要启动一个新的线程来等待 Bp 的请求。
		    // requested_threads_started 表示本进程应请求启动的线程数，
		    // 这个不能超过 max_threads 设置的上限。
		    if (proc->requested_threads + proc->ready_threads == 0 &&
		        proc->requested_threads_started < proc->max_threads &&
		        (thread->looper & (BINDER_LOOPER_STATE_REGISTERED |
		         BINDER_LOOPER_STATE_ENTERED)) /* the user-space code fails to */
		         /*spawn a new thread if we leave this out */) {
		        // 这里发 BR_SPAWN_LOOPER 到用户去创建新线程去了
		        // 然后把请求启动的线程数加1
		        proc->requested_threads++;
		        binder_debug(BINDER_DEBUG_THREADS,
		                 "binder: %d:%d BR_SPAWN_LOOPER\n",
		                 proc->pid, thread->pid);
		        if (put_user(BR_SPAWN_LOOPER, (uint32_t __user *)buffer))
		            return -EFAULT;
		    }
		    return 0;
		}

# 都在运行，唤醒空的怎么处理，累计处理，并且保持有等待的进程？？

其实链表的同步已经处理了


# 不同层次的对应关系以及流动方向

![](http://gityuan.com/images/binder/binder_start_service/binder_ipc_process.jpg)
![](http://gityuan.com/images/binder/binder_start_service/binder_transaction.jpg)
Binder客户端或者服务端向Binder Driver发送的命令都是以BC_开头,例如本文的BC_TRANSACTION和BC_REPLY, 所有Binder Driver向Binder客户端或者服务端发送的命令则都是以BR_开头, 例如本文中的BR_TRANSACTION和BR_REPLY.
只有当BC_TRANSACTION或者BC_REPLY时, 才调用binder_transaction()来处理事务. 并且都会回应调用者一个BINDER_WORK_TRANSACTION_COMPLETE事务, 经过binder_thread_read()会转变成BR_TRANSACTION_COMPLETE.
startService过程便是一个非oneway的过程, 那么oneway的通信过程如下所述.

## oneway

当收到BR_TRANSACTION_COMPLETE则程序返回,有人可能觉得好奇,为何oneway怎么还要等待回应消息? 我举个例子,你就明白了.

你(app进程)要给远方的家人(system_server进程)邮寄一封信(transaction), 你需要通过邮寄员(Binder Driver)来完成.整个过程如下:

你把信交给邮寄员(BC_TRANSACTION);
邮寄员收到信后, 填一张单子给你作为一份回执(BR_TRANSACTION_COMPLETE). 这样你才放心知道邮递员已确定接收信, 否则就这样走了,信到底有没有交到邮递员手里都不知道,这样的通信实在太让人不省心, 长时间收不到远方家人的回信, 无法得知是在路的中途信件丢失呢,还是压根就没有交到邮递员的手里. 所以说oneway也得知道信是投递状态是否成功.
邮递员利用交通工具(Binder Driver),将信交给了你的家人(BR_TRANSACTION);
当你收到回执(BR_TRANSACTION_COMPLETE)时心里也不期待家人回信, 那么这便是一次oneway的通信过程.

如果你希望家人回信, 那便是非oneway的过程,在上述步骤2后并不是直接返回,而是继续等待着收到家人的回信, 经历前3个步骤之后继续执行:

家人收到信后, 立马写了个回信交给邮递员BC_REPLY;
同样,邮递员要写一个回执(BR_TRANSACTION_COMPLETE)给你家人;
邮递员再次利用交通工具(Binder Driver), 将回信成功交到你的手上(BR_REPLY)
这便是一次完成的非oneway通信过程.

oneway与非oneway: 都是需要等待Binder Driver的回应消息BR_TRANSACTION_COMPLETE. 主要区别在于oneway的通信收到BR_TRANSACTION_COMPLETE则返回,而不会再等待BR_REPLY消息的到来. 另外，oneway的binder IPC则接收端无法获取对方的pid.





transact封装 业务层

	BinderDriverCommandProtocol {
	/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
	 BC_TRANSACTION = _IOW_BAD('c', 0, struct binder_transaction_data),
	 BC_REPLY = _IOW_BAD('c', 1, struct binder_transaction_data),
	 BC_ACQUIRE_RESULT = _IOW_BAD('c', 2, int),
	 BC_FREE_BUFFER = _IOW_BAD('c', 3, int),
	/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
	 BC_INCREFS = _IOW_BAD('c', 4, int),
	 BC_ACQUIRE = _IOW_BAD('c', 5, int),
	 BC_RELEASE = _IOW_BAD('c', 6, int),
	 BC_DECREFS = _IOW_BAD('c', 7, int),
	/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
	 BC_INCREFS_DONE = _IOW_BAD('c', 8, struct binder_ptr_cookie),
	 BC_ACQUIRE_DONE = _IOW_BAD('c', 9, struct binder_ptr_cookie),
	 BC_ATTEMPT_ACQUIRE = _IOW_BAD('c', 10, struct binder_pri_desc),
	 BC_REGISTER_LOOPER = _IO('c', 11),
	/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
	 BC_ENTER_LOOPER = _IO('c', 12),
	 BC_EXIT_LOOPER = _IO('c', 13),
	 BC_REQUEST_DEATH_NOTIFICATION = _IOW_BAD('c', 14, struct binder_ptr_cookie),
	 BC_CLEAR_DEATH_NOTIFICATION = _IOW_BAD('c', 15, struct binder_ptr_cookie),
	/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
	 BC_DEAD_BINDER_DONE = _IOW_BAD('c', 16, void *),
	};

	enum BinderDriverReturnProtocol {
	/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
	 BR_ERROR = _IOR_BAD('r', 0, int),
	 BR_OK = _IO('r', 1),
	 BR_TRANSACTION = _IOR_BAD('r', 2, struct binder_transaction_data),
	 BR_REPLY = _IOR_BAD('r', 3, struct binder_transaction_data),
	/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
	 BR_ACQUIRE_RESULT = _IOR_BAD('r', 4, int),
	 BR_DEAD_REPLY = _IO('r', 5),
	 BR_TRANSACTION_COMPLETE = _IO('r', 6),
	 BR_INCREFS = _IOR_BAD('r', 7, struct binder_ptr_cookie),
	/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
	 BR_ACQUIRE = _IOR_BAD('r', 8, struct binder_ptr_cookie),
	 BR_RELEASE = _IOR_BAD('r', 9, struct binder_ptr_cookie),
	 BR_DECREFS = _IOR_BAD('r', 10, struct binder_ptr_cookie),
	 BR_ATTEMPT_ACQUIRE = _IOR_BAD('r', 11, struct binder_pri_ptr_cookie),
	/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
	 BR_NOOP = _IO('r', 12),
	 BR_SPAWN_LOOPER = _IO('r', 13),
	 <!--没用过-->
	 BR_FINISHED = _IO('r', 14),
	 BR_DEAD_BINDER = _IOR_BAD('r', 15, void *),
	/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
	 BR_CLEAR_DEATH_NOTIFICATION_DONE = _IOR_BAD('r', 16, void *),
	 BR_FAILED_REPLY = _IO('r', 17),
	};


talkWithDirver封装

		#define BINDER_WRITE_READ   		_IOWR('b', 1, struct binder_write_read)
		#define	BINDER_SET_IDLE_TIMEOUT		_IOW('b', 3, int64_t)
		#define	BINDER_SET_MAX_THREADS		_IOW('b', 5, size_t)
		#define	BINDER_SET_IDLE_PRIORITY	_IOW('b', 6, int)
		#define	BINDER_SET_CONTEXT_MGR		_IOW('b', 7, int)
		#define	BINDER_THREAD_EXIT		_IOW('b', 8, int)
		#define BINDER_VERSION			_IOWR('b', 9, struct binder_version)

仅仅内核可见的业务层binder_work，封装事务队列，从一个进程转移到另一个进程

	struct binder_work {
		struct list_head entry;
		enum {
			BINDER_WORK_TRANSACTION = 1,
			BINDER_WORK_TRANSACTION_COMPLETE,
			BINDER_WORK_NODE,
			BINDER_WORK_DEAD_BINDER,
			BINDER_WORK_DEAD_BINDER_AND_CLEAR,
			BINDER_WORK_CLEAR_DEATH_NOTIFICATION,
		} type;
	};

[参考文档 http://gityuan.com/2016/09/04/binder-start-service/](http://gityuan.com/2016/09/04/binder-start-service/)

# 并非所有的进程 都能add_Service

 判断进程的uid是否有资格注册名称为name的服务
 
	int svc_can_register(unsigned uid, uint16_t *name)
	{
	    unsigned n;
	    
	    // 谁有权限add_service 0进程，或者 AID_SYSTEM进程
	    if ((uid == 0) || (uid == AID_SYSTEM))
	        return 1;
	
	    for (n = 0; n < sizeof(allowed) / sizeof(allowed[0]); n++)
	        if ((uid == allowed[n].uid) && str16eq(name, allowed[n].name))
	            return 1;
	
	    return 0;
	}
	

*  判断uid是否有资格注册名称为name的服务
*  如果用户是root用户或system用户，不用判断直接可以注册
*  所以，如果Server进程权限不够root和system，那么请记住要在allowed中添加相应的项。
	
	
		static struct {
		    unsigned uid;
		    const char *name;
		} allowed[] = {
		    { AID_MEDIA, "media.audio_flinger" },
		    { AID_MEDIA, "media.log" },
		    { AID_MEDIA, "media.player" },
		    { AID_MEDIA, "media.camera" },
		    { AID_MEDIA, "media.audio_policy" },
		    { AID_DRM,   "drm.drmManager" },
		    { AID_NFC,   "nfc" },
		    { AID_BLUETOOTH, "bluetooth" },
		    { AID_RADIO, "radio.phone" },
		    { AID_RADIO, "radio.sms" },
		    { AID_RADIO, "radio.phonesubinfo" },
		    { AID_RADIO, "radio.simphonebook" },
		/* TODO: remove after phone services are updated: */
		    { AID_RADIO, "phone" },
		    { AID_RADIO, "sip" },
		    { AID_RADIO, "isms" },
		    { AID_RADIO, "iphonesubinfo" },
		    { AID_RADIO, "simphonebook" },
		    { AID_MEDIA, "common_time.clock" },
		    { AID_MEDIA, "common_time.config" },
		    { AID_KEYSTORE, "android.security.keystore" },
		};

# 6.4 小规律

BC_TRANSACTION + BC_REPLY = BR_TRANSACTION_COMPLETE + BR_DEAD_REPLY + BR_FAILED_REPLY
Binder线程只有当本线程的thread->todo队列为空，并且thread->transaction_stack也为空，才会去处理当前进程的事务， 否则会继续处理或等待当前线程的todo队列事务。换句话说，就是只有当前线程的事务;

binder_thread_write: 添加成员到todo队列;
binder_thread_read: 消耗todo队列;

对于处于空闲可用的,或者Ready的binder线程是指停在binder_thread_read()的wait_event地方的Binder线程;
每一次BR_TRANSACTION或者BR_REPLY结束之后都会调用freeBuffer.


#  linux中的用户（UID）、组（GID）、进程（PID)

UserID 不同于UID，uid在安装以及系统启动的时候，就已经确定了

在 Linux 中，一个用户 UID 标示一个给定用户。Linux系统中的用户(UID)分为3类，即普通用户、根用户、系统用户。

      普通用户是指所有使用Linux系统的真实用户，这类用户可以使用用户名及密码登录系统。Linux有着极为详细的权限设置，所以一般来说普通用户只能在其家目录、系统临时目录或其他经过授权的目录中操作，以及操作属于该用户的文件。通常普通用户的UID大于500，因为在添加普通用户时，系统默认用户ID从500开始编号。
      根用户也就是root用户，它的ID是0，也被称为超级用户，root账户拥有对系统的完全控制权：可以修改、删除任何文件，运行任何命令。所以root用户也是系统里面最具危险性的用户，root用户甚至可以在系统正常运行时删除所有文件系统，造成无法挽回的灾难。所以一般情况下，使用root用户登录系统时需要十分小心。
      系统用户是指系统运行时必须有的用户，但并不是指真实的使用者。比如在RedHat或CentOS下运行网站服务时，需要使用系统用户apache来运行httpd进程，而运行MySQL数据库服务时，需要使用系统用户mysql来运行mysqld进程。在RedHat或CentOS下，系统用户的ID范围是1~499。下面给出的示例显示的是目前系统运行的进程，第一列是运行该进程的用户。

       组(GID)又是什么呢？事实上，在Linux下每个用户都至少属于一个组。举个例子：每个学生在学校使用学号来作为标识，而每个学生又都属于某一个班级，这里的学号就相当于UID，而班级就相当于GID。当然了，每个学生可能还会同时参加一些兴趣班，而每个兴趣班也是不同的组。也就是说，每个学生至少属于一个组，也可以同时属于多个组。在Linux下也是一样的道理
       
#  参考文档

[Android Binder 分析——通信模型](http://light3moon.com/2015/01/28/Android%20Binder%20%E5%88%86%E6%9E%90%E2%80%94%E2%80%94%E9%80%9A%E4%BF%A1%E6%A8%A1%E5%9E%8B/)            
[理解 Binder 线程池的管理](https://gold.xitu.io/entry/58197bd9128fe10055a4a51e)       
[Android Binder 分析——多线程支持](http://light3moon.com/2015/01/28/Android%20Binder%20%E5%88%86%E6%9E%90%E2%80%94%E2%80%94%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%94%AF%E6%8C%81/)      
[彻底理解Android Binder通信架构](http://gityuan.com/2016/09/04/binder-start-service/)         
[Android Binder机制の设计与实现4（Binder 协议）](http://blog.csdn.net/xujianqun/article/details/6677862)              
[Android四大组件与进程启动的关系](http://gityuan.com/2016/10/09/app-process-create-2/)          
[binder驱动-------之内存映射篇](http://blog.csdn.net/xiaojsj111/article/details/31422175)           